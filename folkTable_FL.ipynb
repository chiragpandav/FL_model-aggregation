{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a50eedd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from argparse import Namespace\n",
    "from copy import deepcopy\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import preprocessing\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from folktables import ACSDataSource, ACSEmployment,ACSIncome\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Dict, List, OrderedDict, Tuple\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from typing import OrderedDict\n",
    "from typing import Dict, List, OrderedDict, Tuple, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50bfc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "# from argparse import Namespace\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from path import Path\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "from utils.models_folktable import  DeepNet\n",
    "from utils.util import (\n",
    "    LOG_DIR,\n",
    "    TEMP_DIR,\n",
    "    clone_parameters,\n",
    "    fix_random_seed,\n",
    "    get_client_id_indices,\n",
    ")\n",
    "# from client.base import ClientBase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e905b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClientBase:\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: torch.nn.Module,\n",
    "        dataset: str,\n",
    "        batch_size: int,\n",
    "        valset_ratio: float,\n",
    "        testset_ratio: float,\n",
    "        local_epochs: int,\n",
    "        local_lr: float,\n",
    "        logger: Console,\n",
    "        gpu: int,\n",
    "    ):\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if gpu and torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.client_id: int = None\n",
    "        self.valset: DataLoader = None\n",
    "        self.trainset: DataLoader = None\n",
    "        self.testset: DataLoader = None\n",
    "            \n",
    "#         name_of_model = '../../WW_WM_BW.pth'\n",
    "#         init_model = DeepNet()\n",
    "#         init_model.load_state_dict(torch.load(name_of_model))\n",
    "             \n",
    "        # need to change\n",
    "#         self.model: torch.nn.Module = init_model\n",
    "        self.model: torch.nn.Module = deepcopy(backbone)\n",
    "            \n",
    "        self.optimizer: torch.optim.Optimizer = torch.optim.SGD(\n",
    "            self.model.parameters(), lr=local_lr\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "        self.valset_ratio = valset_ratio\n",
    "        self.testset_ratio = testset_ratio\n",
    "        self.local_epochs = local_epochs\n",
    "        self.local_lr = local_lr\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.logger = logger\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        size = 0\n",
    "        loss = 0\n",
    "        correct = 0\n",
    "        for x, y in self.testset:\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "            logits = self.model(x)\n",
    "\n",
    "            loss += self.criterion(logits, y)\n",
    "\n",
    "            pred = torch.softmax(logits, -1).argmax(-1)\n",
    "\n",
    "            correct += (pred == y).int().sum()\n",
    "\n",
    "            size += y.size(-1)\n",
    "\n",
    "        acc = correct / size * 100.0\n",
    "        loss = loss / len(self.testset)\n",
    "        return loss, acc\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def _train(self):\n",
    "        pass\n",
    "\n",
    "    def get_client_local_dataset(self):\n",
    "        datasets = get_dataloader(\n",
    "            self.dataset,\n",
    "            self.client_id,\n",
    "            self.batch_size,\n",
    "            self.valset_ratio,\n",
    "            self.testset_ratio,\n",
    "        )\n",
    "        self.trainset = datasets[\"train\"]\n",
    "        self.valset = datasets[\"val\"]\n",
    "        self.testset = datasets[\"test\"]\n",
    "\n",
    "    def _log_while_training(self, evaluate=True, verbose=False):\n",
    "        def _log_and_train(*args, **kwargs):\n",
    "            loss_before = 0\n",
    "            loss_after = 0\n",
    "            acc_before = 0\n",
    "            acc_after = 0\n",
    "            if evaluate:\n",
    "                loss_before, acc_before = self.evaluate()\n",
    "\n",
    "            res = self._train(*args, **kwargs)\n",
    "\n",
    "            if evaluate:\n",
    "                loss_after, acc_after = self.evaluate()\n",
    "\n",
    "            if verbose:\n",
    "                self.logger.log(\n",
    "                    \"client [{}]   [bold red]loss: {:.4f} -> {:.4f}    [bold blue]accuracy: {:.2f}% -> {:.2f}%\".format(\n",
    "                        self.client_id, loss_before, loss_after, acc_before, acc_after\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            eval_stats = {\n",
    "                \"loss_before\": loss_before,\n",
    "                \"loss_after\": loss_after,\n",
    "                \"acc_before\": acc_before,\n",
    "                \"acc_after\": acc_after,\n",
    "            }\n",
    "            return res, eval_stats\n",
    "\n",
    "        return _log_and_train\n",
    "\n",
    "    def set_parameters(self, model_params: OrderedDict):\n",
    "        self.model.load_state_dict(model_params, strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5055145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import OrderedDict\n",
    "\n",
    "import torch\n",
    "from rich.console import Console\n",
    "from utils.util import clone_parameters\n",
    "\n",
    "# from client.base import ClientBase\n",
    "\n",
    "class pFedLAClient(ClientBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: torch.nn.Module,        \n",
    "        dataset: str,\n",
    "        batch_size: int,\n",
    "        valset_ratio: float,\n",
    "        testset_ratio: float,\n",
    "        local_epochs: int,\n",
    "        local_lr: float,\n",
    "        logger: Console,\n",
    "        gpu: int,\n",
    "    ):\n",
    "        super(pFedLAClient, self).__init__(\n",
    "            backbone,\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            valset_ratio,\n",
    "            testset_ratio,\n",
    "            local_epochs,\n",
    "            logger,\n",
    "            local_lr,\n",
    "            gpu,\n",
    "        )\n",
    "        \n",
    "    def train(\n",
    "        self,\n",
    "        client_id: int,\n",
    "        model_params: OrderedDict[str, torch.Tensor],\n",
    "        verbose=True,\n",
    "    ):\n",
    "        self.client_id = client_id\n",
    "        self.set_parameters(model_params)\n",
    "        self.get_client_local_dataset()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        res, stats = self._log_while_training(evaluate=True, verbose=verbose)()\n",
    "        self.model.cpu()\n",
    "        return res, stats\n",
    "        \n",
    "    def _train(self):\n",
    "        self.model.train()\n",
    "        frz_model_params = clone_parameters(self.model)\n",
    "\n",
    "        for _ in range(self.local_epochs):\n",
    "            for x, y in self.trainset:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                logits = self.model(x)\n",
    "\n",
    "                loss = self.criterion(logits, y)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        delta = OrderedDict(\n",
    "            {\n",
    "                k: p1 - p0 \n",
    "                for (k, p1), p0 in zip(\n",
    "                    self.model.state_dict(keep_vars=True).items(),\n",
    "                    frz_model_params.values(),\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return delta\n",
    "    \n",
    "    def test(\n",
    "        self, client_id: int, model_params: OrderedDict[str, torch.Tensor],\n",
    "    ):\n",
    "        self.client_id = client_id\n",
    "        self.set_parameters(model_params)\n",
    "        self.get_client_local_dataset()\n",
    "        self.model.to(self.device)\n",
    "        loss, acc = self.evaluate()\n",
    "        dummy_diff = OrderedDict(\n",
    "            {\n",
    "                name: torch.zeros_like(param)\n",
    "                for name, param in self.model.state_dict().items()\n",
    "            }\n",
    "        )\n",
    "        self.model.cpu()\n",
    "        stats = {\"loss\": loss, \"acc\": acc}\n",
    "        return dummy_diff, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74126905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "from argparse import Namespace\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from path import Path\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.util import (\n",
    "    LOG_DIR,\n",
    "    TEMP_DIR,\n",
    "    clone_parameters,\n",
    "    fix_random_seed,\n",
    "    get_client_id_indices,\n",
    ")\n",
    "\n",
    "class ServerBase:\n",
    "    def __init__(self, args: Namespace, algo: str):\n",
    "        self.algo = algo\n",
    "        self.args = args\n",
    "        \n",
    "        # default log file format\n",
    "        self.log_name = \"{}_{}_{}_{}.html\".format(\n",
    "            self.algo,\n",
    "            self.args.dataset,\n",
    "            self.args.global_epochs,\n",
    "            self.args.local_epochs,\n",
    "        )\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        fix_random_seed(5)\n",
    "        self.global_epochs=5\n",
    "        \n",
    "#         self.backbone= (DeepNet)   \n",
    "        \n",
    "        self.backbone = (\n",
    "            CNNWithBatchNorm\n",
    "            if self.args.dataset in [\"cifar10\", \"cifar100\"]\n",
    "            else DeepNet\n",
    "        )\n",
    "        \n",
    "        self.logger = Console(record=True, log_path=False, log_time=False,)\n",
    "        self.client_id_indices, self.client_num_in_total = [0,1,2,3], 3\n",
    "        \n",
    "        self.temp_dir =\"D:/Download/pythonProject/HiWi/pFedLA_Folktable/temp/my_model\"\n",
    "\n",
    "\n",
    "#         name_of_model = '../../WW_WM_BW.pth'\n",
    "        #  self.temp_dir = TEMP_DIR / self.algo\n",
    "        if not os.path.isdir(self.temp_dir):\n",
    "            os.makedirs(self.temp_dir)\n",
    "            \n",
    "            \n",
    "#         init_model = DeepNet()\n",
    "#         init_model.load_state_dict(torch.load(name_of_model))\n",
    "        \n",
    "#         _dummy_model = init_model\n",
    "        \n",
    "#         _dummy_model = self.backbone(DeepNet)\n",
    "        _dummy_model = DeepNet()\n",
    "        print(\"_dummy_model:: chriag\\n\",_dummy_model)\n",
    "#         print(type(_dummy_model))\n",
    "        \n",
    "        passed_epoch = 0\n",
    "        self.global_params_dict: OrderedDict[str : torch.Tensor] = None\n",
    "        \n",
    "#         if os.listdir(self.temp_dir) != []:\n",
    "#             if os.path.exists(os.path.join(self.temp_dir, \"global_model.pt\")):\n",
    "# #             if os.path.exists(self.temp_dir / \"global_model.pt\"):\n",
    "                \n",
    "#                 self.global_params_dict = torch.load(self.temp_dir / \"global_model.pt\")\n",
    "#                 self.logger.log(\"Find existed global model...\")\n",
    "\n",
    "# #           if os.path.exists(self.temp_dir / \"epoch.pkl\"):  \n",
    "# #               with open(self.temp_dir / \"epoch.pkl\", \"rb\") as f:\n",
    "#             if os.path.exists(os.path.join(self.temp_dir, \"epoch.pkl\")):  \n",
    "#                 with open(os.path.join(self.temp_dir, \"epoch.pkl\"), \"rb\") as f:\n",
    "# #               \n",
    "#                     passed_epoch = pickle.load(f)\n",
    "#                 self.logger.log(f\"Have run {passed_epoch} epochs already.\",)\n",
    "#         else:\n",
    "#             self.global_params_dict = OrderedDict(_dummy_model.state_dict())\n",
    "            \n",
    "        self.global_params_dict = OrderedDict(_dummy_model.state_dict())\n",
    "\n",
    "#         self.global_epochs = self.args.global_epochs - passed_epoch\n",
    "        self.global_epochs = self.global_epochs - passed_epoch\n",
    "    \n",
    "        self.logger.log(\"Backbone:\", _dummy_model)\n",
    "\n",
    "        self.trainer: ClientBase = None\n",
    "        self.all_clients_stats = {i: {} for i in self.client_id_indices}\n",
    "           \n",
    "            \n",
    "            \n",
    "    def train(self):\n",
    "\n",
    "        print(\"In server class \\n\")\n",
    "        self.logger.log(\"=\" * 30, \"TRAINING\", \"=\" * 30, style=\"bold green\")\n",
    "        progress_bar = (\n",
    "            track(\n",
    "                range(self.global_epochs),\n",
    "                \"[bold green]Training...\",\n",
    "                console=self.logger,\n",
    "            )\n",
    "            if not self.args.log\n",
    "            else tqdm(range(self.global_epochs), \"Training...\")\n",
    "        )\n",
    "        for E in progress_bar:\n",
    "\n",
    "#             if E % self.args.verbose_gap == 0:\n",
    "#                 self.logger.log(\"=\" * 30, f\"ROUND: {E}\", \"=\" * 30)\n",
    "\n",
    "            selected_clients = random.sample(\n",
    "                self.client_id_indices, self.args.client_num_per_round\n",
    "            )\n",
    "            \n",
    "            updated_params_cache = []\n",
    "            weights_cache = []\n",
    "\n",
    "            for client_id in selected_clients:\n",
    "                client_local_params = clone_parameters(self.global_params_dict)\n",
    "                (updated_params, weight), stats = self.trainer.train(\n",
    "                    client_id=client_id,\n",
    "                    model_params=client_local_params,\n",
    "                    verbose=(E % self.args.verbose_gap) == 0,\n",
    "                )\n",
    "\n",
    "                updated_params_cache.append(updated_params)\n",
    "                weights_cache.append(weight)\n",
    "                self.all_clients_stats[client_id][f\"ROUND: {E}\"] = (\n",
    "                    f\"{stats['loss_before']:.4f} -> {stats['loss_after']:.4f}\",\n",
    "                )\n",
    "\n",
    "            self.aggregate_parameters(updated_params_cache, weights_cache)\n",
    "\n",
    "            if E % self.args.save_period == 0:\n",
    "                torch.save(\n",
    "                    self.global_params_dict, self.temp_dir / \"global_model.pt\",\n",
    "                )\n",
    "                with open(self.temp_dir / \"epoch.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(E, f)\n",
    "        self.logger.log(self.all_clients_stats)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def aggregate_parameters(self, updated_params_cache, weights_cache):\n",
    "        weight_sum = sum(weights_cache)\n",
    "        weights = torch.tensor(weights_cache, device=self.device) / weight_sum\n",
    "\n",
    "        aggregated_params = []\n",
    "\n",
    "        for params in zip(*updated_params_cache):\n",
    "            aggregated_params.append(\n",
    "                torch.sum(weights * torch.stack(params, dim=-1), dim=-1)\n",
    "            )\n",
    "\n",
    "        self.global_params_dict = OrderedDict(\n",
    "            zip(self.global_params_dict.keys(), aggregated_params)\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "    def test(self) -> None:\n",
    "        self.logger.log(\"=\" * 30, \"TESTING\", \"=\" * 30, style=\"bold blue\")\n",
    "        all_loss = []\n",
    "        all_acc = []\n",
    "        for client_id in track(\n",
    "            self.client_id_indices,\n",
    "            \"[bold blue]Testing...\",\n",
    "            console=self.logger,\n",
    "            disable=self.args.log,\n",
    "        ):\n",
    "            client_local_params = clone_parameters(self.global_params_dict)\n",
    "            stats = self.trainer.test(\n",
    "                client_id=client_id, model_params=client_local_params,\n",
    "            )\n",
    "\n",
    "            self.logger.log(\n",
    "                f\"client [{client_id}]  [red]loss: {stats['loss']:.4f}    [magenta]accuracy: {stats['acc']:.2f}%\"\n",
    "            )\n",
    "            all_loss.append(stats[\"loss\"])\n",
    "            all_acc.append(stats[\"acc\"])\n",
    "\n",
    "        self.logger.log(\"=\" * 20, \"RESULTS\", \"=\" * 20, style=\"bold green\")\n",
    "        self.logger.log(\n",
    "            \"loss: {:.4f}    accuracy: {:.2f}%\".format(\n",
    "                sum(all_loss) / len(all_loss), sum(all_acc) / len(all_acc),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "#         self.logger.log(\"Arguments:\", dict(self.args._get_kwargs()))\n",
    "        self.train()\n",
    "        self.test()\n",
    "        if self.args.log:\n",
    "            if not os.path.isdir(LOG_DIR):\n",
    "                os.mkdir(LOG_DIR)\n",
    "            self.logger.save_html(LOG_DIR / self.log_name)\n",
    "\n",
    "        # delete all temporary files\n",
    "        if os.listdir(self.temp_dir) != []:\n",
    "            os.system(f\"rm -rf {self.temp_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aff4856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.empty(out_features))\n",
    "\n",
    "        nn.init.uniform_(self.weight)\n",
    "        nn.init.constant_(self.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight, self.bias)\n",
    "\n",
    "    \n",
    "\n",
    "class HyperNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        backbone: nn.Module,\n",
    "        client_num: int=4,\n",
    "        K: int=2,\n",
    "        gpu=True,\n",
    "    ):\n",
    "    \n",
    "        super(HyperNetwork, self).__init__()\n",
    "        self.device = torch.device(\n",
    "                \"cuda\" if gpu and torch.cuda.is_available() else \"cpu\"\n",
    "            )\n",
    "        \n",
    "        self.K = K\n",
    "        self.client_num = client_num\n",
    "        self.embedding = nn.Embedding(client_num, embedding_dim, device=self.device)\n",
    "        self.blocks_name = set(n.split(\".\")[0] for n, _ in backbone.named_parameters())\n",
    "        self.cache_dir =  \"pkl_files/hn\"  # put dir here\n",
    "        \n",
    "        if os.listdir(self.cache_dir) != client_num:\n",
    "            \n",
    "            for client_id in range(client_num):\n",
    "#                 with open(self.cache_dir / f\"{client_id}.pkl\", \"wb\") as f:\n",
    "                with open(os.path.join(self.cache_dir, f\"{client_id}.pkl\"), \"wb\") as f:\n",
    "\n",
    "                    pickle.dump(\n",
    "                        {\n",
    "                            \"mlp\": nn.Sequential(\n",
    "                                nn.Linear(embedding_dim, hidden_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(hidden_dim, hidden_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(hidden_dim, hidden_dim),\n",
    "                                nn.ReLU(),\n",
    "                            ),\n",
    "                            \n",
    "                            \"fc\": {\n",
    "                                name: Linear(hidden_dim, client_num)\n",
    "                                for name in self.blocks_name\n",
    "                            },\n",
    "                        },\n",
    "                        f,\n",
    "                    )\n",
    "\n",
    "        # for tracking the current client's hn parameters\n",
    "        self.current_client_id: int = None\n",
    "        self.mlp: nn.Sequential = None\n",
    "        self.fc_layers: Dict[str, Linear] = {}\n",
    "        self.retain_blocks: List[str] = []\n",
    "            \n",
    "        print(\"HypterNetwork\")\n",
    "        \n",
    "    def mlp_parameters(self) -> List[nn.Parameter]:\n",
    "            print(\"self.mlp.parameters():: \", self.mlp)            \n",
    "            return list(filter(lambda p: p.requires_grad, self.mlp.parameters()))   \n",
    "        \n",
    "        \n",
    "    def fc_layer_parameters(self) -> List[nn.Parameter]:\n",
    "        params_list = []\n",
    "        for block, fc in self.fc_layers.items():\n",
    "            if block not in self.retain_blocks:\n",
    "                params_list += list(filter(lambda p: p.requires_grad, fc.parameters()))\n",
    "\n",
    "        return params_list\n",
    "\n",
    "    def emd_parameters(self) -> List[nn.Parameter]:\n",
    "        return list(self.embedding.parameters())\n",
    "        \n",
    "    \n",
    "    def forward(self, client_id: int) -> Tuple[Dict[str, torch.Tensor], List[str]]:\n",
    "        self.current_client_id = client_id\n",
    "\n",
    "        print(\" self.current_client_id : \",self.current_client_id,\"\\n\")\n",
    "\n",
    "        self.retain_blocks = []\n",
    "        emd = self.embedding(\n",
    "            torch.tensor(client_id, dtype=torch.long, device=self.device)\n",
    "        )\n",
    "        self.load_hn()\n",
    "\n",
    "        feature = self.mlp(emd)\n",
    "\n",
    "        print(\"TEMP_DIR:: \",TEMP_DIR )\n",
    "\n",
    "        # print(\" features : \",feature,\"\\n\")\n",
    "\n",
    "        alpha = {\n",
    "            block: F.relu(self.fc_layers[block](feature)) for block in self.blocks_name\n",
    "        }\n",
    "\n",
    "        # print(\"  alpha: \",alpha,\"\\n\")\n",
    "\n",
    "        default_weight = torch.tensor(\n",
    "            [i == client_id for i in range(self.client_num)],\n",
    "            dtype=torch.float,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        print(\" self.k \", self.K,\"\\n\")\n",
    "        # i set K =2 in arg.py\n",
    "\n",
    "        if self.K > 0:  # HeurpFedLA\n",
    "            \n",
    "            blocks_name = []\n",
    "            self_weights = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for name, weight in alpha.items():\n",
    "\n",
    "                    # print(\"-: NAME AND WEIGHT : \", name, weight,\"\\n\")\n",
    "\n",
    "                    blocks_name.append(name)\n",
    "                    self_weights.append(weight[client_id])\n",
    "\n",
    "                # not in the Loop\n",
    "                _, topk_weights_idx = torch.topk(torch.tensor(self_weights), self.K)\n",
    "\n",
    "                print(\"  topk_weights_idx \",topk_weights_idx ,\"\\n\")\n",
    "                \n",
    "            for i in topk_weights_idx:\n",
    "                # print(\" topk_weights_idx I  \",i,\"\\n\")\n",
    "                print(\" blocks_name[i] Topk \",blocks_name[i],\"\\n\")\n",
    "                # print(\" default_weight[i] Topk \",default_weight,\"\\n\")\n",
    "\n",
    "                alpha[blocks_name[i]] = default_weight\n",
    "                self.retain_blocks.append(blocks_name[i])\n",
    "\n",
    "        return alpha, self.retain_blocks\n",
    "    \n",
    "    \n",
    "    \n",
    "    def save_hn(self):\n",
    "            for block, param in self.fc_layers.items():\n",
    "                self.fc_layers[block] = param.cpu()\n",
    "            with open(self.cache_dir / f\"{self.current_client_id}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(\n",
    "                    {\"mlp\": self.mlp.cpu(), \"fc\": self.fc_layers}, f,\n",
    "                )\n",
    "            self.mlp = None\n",
    "            self.fc_layers = {}\n",
    "            self.current_client_id = None\n",
    "\n",
    "    def load_hn(self) -> Tuple[nn.Sequential, OrderedDict[str, Linear]]:\n",
    "        with open(os.path.join(self.cache_dir, f\"{self.current_client_id}.pkl\"), \"rb\") as f:\n",
    "        \n",
    "#         with open(self.cache_dir / f\"{self.current_client_id}.pkl\", \"rb\") as f:\n",
    "            parameters = pickle.load(f)\n",
    "        self.mlp = parameters[\"mlp\"].to(self.device)\n",
    "        for block, param in parameters[\"fc\"].items():\n",
    "            self.fc_layers[block] = param.to(self.device)\n",
    "\n",
    "    def clean_models(self):\n",
    "        if os.path.isdir(self.cache_dir):\n",
    "            os.system(f\"rm -rf {self.cache_dir}\")\n",
    "            \n",
    "            \n",
    "            \n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 14 : input shape\n",
    "        self.layer1 = nn.Linear(14, 512)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(256, 60)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133d7f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args1 = Namespace(\n",
    "    k=2,\n",
    "    global_epochs=5,\n",
    "    local_epochs=5,\n",
    "    local_lr=1e-2,\n",
    "    hn_lr=5e-3,\n",
    "    verbose_gap=20,\n",
    "    embedding_dim=10,\n",
    "    hidden_dim=10,\n",
    "    dataset=\"no\",\n",
    "    batch_size=32,\n",
    "    valset_ratio=0.0,\n",
    "    testset_ratio=0.3,\n",
    "    gpu=1,\n",
    "    log=0,\n",
    "    seed=5,\n",
    "    client_num_per_round=4,\n",
    "    save_period=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "652d5744",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_dummy_model:: chriag\n",
      " DeepNet(\n",
      "  (layer1): Linear(in_features=14, out_features=512, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (layer2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (layer3): Linear(in_features=256, out_features=60, bias=True)\n",
      "  (act3): ReLU()\n",
      "  (output): Linear(in_features=60, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Backbone: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DeepNet</span><span style=\"font-weight: bold\">(</span>                                                                                                 \n",
       "  <span style=\"font-weight: bold\">(</span>layer1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>                                                    \n",
       "  <span style=\"font-weight: bold\">(</span>act1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>                                                                                                   \n",
       "  <span style=\"font-weight: bold\">(</span>dropout1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                                                        \n",
       "  <span style=\"font-weight: bold\">(</span>layer2<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>                                                   \n",
       "  <span style=\"font-weight: bold\">(</span>act2<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>                                                                                                   \n",
       "  <span style=\"font-weight: bold\">(</span>layer3<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>                                                    \n",
       "  <span style=\"font-weight: bold\">(</span>act3<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>                                                                                                   \n",
       "  <span style=\"font-weight: bold\">(</span>output<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>                                                      \n",
       "  <span style=\"font-weight: bold\">(</span>sigmoid<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Sigmoid</span><span style=\"font-weight: bold\">()</span>                                                                                             \n",
       "<span style=\"font-weight: bold\">)</span>                                                                                                                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Backbone: \u001b[1;35mDeepNet\u001b[0m\u001b[1m(\u001b[0m                                                                                                 \n",
       "  \u001b[1m(\u001b[0mlayer1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m14\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                                    \n",
       "  \u001b[1m(\u001b[0mact1\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                                                                   \n",
       "  \u001b[1m(\u001b[0mdropout1\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                                                        \n",
       "  \u001b[1m(\u001b[0mlayer2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                                   \n",
       "  \u001b[1m(\u001b[0mact2\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                                                                   \n",
       "  \u001b[1m(\u001b[0mlayer3\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m60\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                                    \n",
       "  \u001b[1m(\u001b[0mact3\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                                                                   \n",
       "  \u001b[1m(\u001b[0moutput\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m60\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                                      \n",
       "  \u001b[1m(\u001b[0msigmoid\u001b[1m)\u001b[0m: \u001b[1;35mSigmoid\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                                                             \n",
       "\u001b[1m)\u001b[0m                                                                                                                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.DeepNet'>\n",
      "HypterNetwork\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">============================== TRAINING ==============================                                             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m============================== TRAINING ==============================                                             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b28621886bd4a6da16730b258a9d437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">============================== ROUND: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> ==============================                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "============================== ROUND: \u001b[1;36m0\u001b[0m ==============================                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> self.current_client_id :  2 \n",
       "</pre>\n"
      ],
      "text/plain": [
       " self.current_client_id :  2 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TEMP_DIR::  D:\\Download\\pythonProject\\HiWi\\pFedLA_Folktable\\temp\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TEMP_DIR::  D:\\Download\\pythonProject\\HiWi\\pFedLA_Folktable\\temp\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> self.k  2 \n",
       "</pre>\n"
      ],
      "text/plain": [
       " self.k  2 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  topk_weights_idx  tensor([0, 1]) \n",
       "</pre>\n"
      ],
      "text/plain": [
       "  topk_weights_idx  tensor([0, 1]) \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> blocks_name[i] Topk  layer2 \n",
       "</pre>\n"
      ],
      "text/plain": [
       " blocks_name[i] Topk  layer2 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> blocks_name[i] Topk  layer3 \n",
       "</pre>\n"
      ],
      "text/plain": [
       " blocks_name[i] Topk  layer3 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">retain_blocks :  ['layer2', 'layer3'] \n",
       "</pre>\n"
      ],
      "text/plain": [
       "retain_blocks :  ['layer2', 'layer3'] \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DeepNet:\n\tsize mismatch for layer1.weight: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([512, 14]).\n\tsize mismatch for layer1.bias: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer2.weight: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for layer2.bias: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.weight: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([60, 256]).\n\tsize mismatch for layer3.bias: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([1, 60]).\n\tsize mismatch for output.bias: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 277\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    276\u001b[0m     server \u001b[38;5;241m=\u001b[39m pFedLAServer()\n\u001b[1;32m--> 277\u001b[0m     \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 271\u001b[0m, in \u001b[0;36mpFedLAServer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 271\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# clean out all HNs\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypernet\u001b[38;5;241m.\u001b[39mclean_models()\n",
      "Cell \u001b[1;32mIn[5], line 199\u001b[0m, in \u001b[0;36mServerBase.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m#         self.logger.log(\"Arguments:\", dict(self.args._get_kwargs()))\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest()\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mlog:\n",
      "Cell \u001b[1;32mIn[8], line 136\u001b[0m, in \u001b[0;36mpFedLAServer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client_id \u001b[38;5;129;01min\u001b[39;00m selected_clients:\n\u001b[0;32m    131\u001b[0m     (\n\u001b[0;32m    132\u001b[0m         client_local_params,\n\u001b[0;32m    133\u001b[0m         retain_blocks,\n\u001b[0;32m    134\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_client_model_parameters(client_id)\n\u001b[1;32m--> 136\u001b[0m     diff, stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_local_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose_gap\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_clients_stats[client_id][\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROUND: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretain \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretain_blocks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_before\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_after\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    144\u001b[0m     )\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_hypernetwork(client_id, diff, retain_blocks)\n",
      "Cell \u001b[1;32mIn[4], line 42\u001b[0m, in \u001b[0;36mpFedLAClient.train\u001b[1;34m(self, client_id, model_params, verbose)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     37\u001b[0m     client_id: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m     38\u001b[0m     model_params: OrderedDict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m     39\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     40\u001b[0m ):\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_id \u001b[38;5;241m=\u001b[39m client_id\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_client_local_dataset()\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[3], line 114\u001b[0m, in \u001b[0;36mClientBase.set_parameters\u001b[1;34m(self, model_params)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_params: OrderedDict):\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1666\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1667\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1668\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1672\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DeepNet:\n\tsize mismatch for layer1.weight: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([512, 14]).\n\tsize mismatch for layer1.bias: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer2.weight: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for layer2.bias: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.weight: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([60, 256]).\n\tsize mismatch for layer3.bias: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([60]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([1, 60]).\n\tsize mismatch for output.bias: copying a param with shape torch.Size([]) from checkpoint, the shape in current model is torch.Size([1])."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from typing import List, OrderedDict, Tuple\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "\n",
    "# from server.base import ServerBase\n",
    "# from client.pFedLA import pFedLAClient\n",
    "from tqdm import tqdm\n",
    "# from utils.args import get_pFedLA_args\n",
    "\n",
    "\n",
    "class pFedLAServer(ServerBase):\n",
    "    def __init__(self):\n",
    "        super(pFedLAServer, self).__init__(args1, \"pFedLA\")\n",
    "        \n",
    "        self.log_name = \"{}_{}_{}_{}_{}.html\".format(\n",
    "            self.algo,\n",
    "            self.args.dataset,\n",
    "            self.args.global_epochs,\n",
    "            self.args.local_epochs,\n",
    "            self.args.k,\n",
    "        )\n",
    "        \n",
    "        bm=\"..\\\\1_BM_WM_WW.pth\"\n",
    "        wm=\"..\\\\2_WM_BM_BW.pth\"\n",
    "        ww=\"..\\\\3_WW_BW_BM.pth\"\n",
    "        bw=\"..\\\\4_WW_WM_BW.pth\"\n",
    "\n",
    "        bm_state_dict_values = torch.load(bm).values()\n",
    "        wm_state_dict_values = torch.load(wm).values()\n",
    "        ww_state_dict_values = torch.load(ww).values()\n",
    "        bw_state_dict_values = torch.load(bw).values()\n",
    "\n",
    "\n",
    "        bm_values_list = list(bm_state_dict_values)\n",
    "        wm_values_list = list(wm_state_dict_values)\n",
    "        ww_values_list = list(ww_state_dict_values)\n",
    "        bw_values_list = list(bw_state_dict_values)\n",
    "\n",
    "        self.client_model_params_list = [bm_values_list, wm_values_list, ww_values_list, bw_values_list]\n",
    "        \n",
    "#         if self.global_params_dict is not None:\n",
    "#             del self.global_params_dict  # pFedLA don't have global model\n",
    "\n",
    "#         if os.listdir(self.temp_dir) != []:\n",
    "                        \n",
    "# #             if os.path.exists(self.temp_dir / \"clients_model.pt\"):\n",
    "#             if os.path.exists(os.path.join(self.temp_dir, \"clients_model.pt\")):\n",
    "#                 self.client_model_params_list = torch.load(\n",
    "#                     self.temp_dir / \"clients_model.pt\"\n",
    "#                 )\n",
    "#                 self.logger.log(\"Find existed clients model...\")\n",
    "#         else:\n",
    "#             self.logger.log(\"Initializing clients model...\")\n",
    "            \n",
    "#             self.client_model_params_list = [\n",
    "#                 list(self.backbone(self.args.dataset).state_dict().values())\n",
    "#                 for _ in range(self.client_num_in_total)\n",
    "#             ]\n",
    "            \n",
    "#         _dummy_model = self.backbone(DeepNet)\n",
    "\n",
    "\n",
    "        _dummy_model = DeepNet()\n",
    "        print(type(_dummy_model))\n",
    "        \n",
    "#         _dummy_model = self.backbone(self.args.dataset)\n",
    "\n",
    "        \n",
    "#         name_of_model = '../../WW_WM_BW.pth'\n",
    "#         init_model = DeepNet()\n",
    "#         init_model.load_state_dict(torch.load(name_of_model))\n",
    "        \n",
    "        #need to change\n",
    "#         _dummy_model = init_model\n",
    "#         self.client_model_params_list = init_model.load_state_dict(torch.load(name_of_model))\n",
    "        \n",
    "        self.hypernet = HyperNetwork(\n",
    "            client_num=4,\n",
    "            backbone=_dummy_model,\n",
    "            embedding_dim=10,\n",
    "            hidden_dim=10,\n",
    "            K=2,\n",
    "            gpu=True,\n",
    "        )\n",
    "        \n",
    "        self.trainer = pFedLAClient(\n",
    "            backbone=_dummy_model,\n",
    "            dataset=self.args.dataset,\n",
    "            batch_size=32,\n",
    "            valset_ratio=0.0,\n",
    "            testset_ratio=0.3,\n",
    "            local_epochs=5,\n",
    "            local_lr=1e-2,\n",
    "            logger=0,\n",
    "            gpu=True,\n",
    "        )\n",
    "        \n",
    "        self.all_params_name = [name for name in _dummy_model.state_dict().keys()]\n",
    "        \n",
    "        self.trainable_params_name = [\n",
    "                name\n",
    "                for name, param in _dummy_model.state_dict(keep_vars=True).items()\n",
    "                if param.requires_grad\n",
    "            ]\n",
    "\n",
    "    def train(self) -> None:\n",
    "        self.logger.log(\"=\" * 30, \"TRAINING\", \"=\" * 30, style=\"bold green\")\n",
    "        progress_bar = (\n",
    "            track(\n",
    "                range(self.global_epochs),\n",
    "                \"[bold green]Training...\",\n",
    "                console=self.logger,\n",
    "            )\n",
    "            if not self.args.log\n",
    "            else tqdm(range(self.global_epochs), \"Training...\")\n",
    "        )        \n",
    "        \n",
    "        for E in progress_bar:\n",
    "\n",
    "            if E % self.args.verbose_gap == 0:\n",
    "                self.logger.log(\"=\" * 30, f\"ROUND: {E}\", \"=\" * 30)\n",
    "\n",
    "            selected_clients = random.sample(\n",
    "                self.client_id_indices, self.args.client_num_per_round\n",
    "            )\n",
    "            for client_id in selected_clients:\n",
    "                (\n",
    "                    client_local_params,\n",
    "                    retain_blocks,\n",
    "                ) = self.generate_client_model_parameters(client_id)\n",
    "\n",
    "                diff, stats = self.trainer.train(\n",
    "                    client_id=client_id,\n",
    "                    model_params=client_local_params,\n",
    "                    verbose=(E % self.args.verbose_gap) == 1,\n",
    "                )\n",
    "                \n",
    "                self.all_clients_stats[client_id][f\"ROUND: {E}\"] = (\n",
    "                    f\"retain {retain_blocks}, {stats['loss_before']:.4f} -> {stats['loss_after']:.4f}\",\n",
    "                )\n",
    "\n",
    "                self.update_hypernetwork(client_id, diff, retain_blocks)\n",
    "\n",
    "                self.update_client_model_parameters(client_id, diff)\n",
    "\n",
    "            if E % self.args.save_period == 0:\n",
    "                torch.save(\n",
    "                    self.client_model_params_list, self.temp_dir / \"clients_model.pt\",\n",
    "                )\n",
    "                with open(self.temp_dir / \"epoch.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(E, f)\n",
    "        self.logger.log(self.all_clients_stats)\n",
    "    \n",
    "    def generate_client_model_parameters(\n",
    "            self, client_id: int\n",
    "        ) -> Tuple[OrderedDict[str, torch.Tensor], List[str]]:\n",
    "        \n",
    "        \n",
    "            layer_params_dict = dict(\n",
    "                zip(self.all_params_name, list(zip(*self.client_model_params_list)))\n",
    "            )\n",
    "\n",
    "            alpha, retain_blocks = self.hypernet(client_id)\n",
    "\n",
    "            aggregated_parameters = {}\n",
    "            default_weight = torch.tensor(\n",
    "                [i == client_id for i in range(self.client_num_in_total)],\n",
    "                dtype=torch.float,\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "\n",
    "            for name in self.all_params_name:\n",
    "\n",
    "                if name in self.trainable_params_name:\n",
    "                    a = alpha[name.split(\".\")[0]]\n",
    "\n",
    "                else:\n",
    "                    a = default_weight\n",
    "\n",
    "#                 print(\"From hypernet:  alpha \", alpha,\"\\n\")\n",
    "\n",
    "                if a.sum() == 0:\n",
    "                    self.logger.log(self.all_clients_stats)\n",
    "                    raise RuntimeError(\n",
    "                        f\"client [{client_id}]'s {name.split('.')[0]} alpha is a all 0 vector\"\n",
    "                    )\n",
    "\n",
    "\n",
    "                # print(\"layer_params_dict[name]\", name,\" :: \",layer_params_dict[name][0],\"\\n\")\n",
    "\n",
    "#                 aggregated_parameters[name] = torch.sum(\n",
    "#                     a\n",
    "#                     / a.sum()\n",
    "#                     * torch.stack(layer_params_dict[name], dim=-1).to(self.device),\n",
    "#                     dim=-1,\n",
    "#                 )\n",
    "                \n",
    "                aggregated_parameters[name] = torch.sum(a/ a.sum(),dim=-1)\n",
    "\n",
    "            # in the begining, its  client_model_params_list.value \n",
    "            ##putting values to  particular client's weights\n",
    "            self.client_model_params_list[client_id] = list(aggregated_parameters.values())\n",
    "\n",
    "            print(\"retain_blocks : \" , retain_blocks,\"\\n\")\n",
    "            \n",
    "            return aggregated_parameters, retain_blocks\n",
    "        \n",
    "    def update_hypernetwork(\n",
    "        self,\n",
    "        client_id: int,\n",
    "        diff: OrderedDict[str, torch.Tensor],\n",
    "        retain_blocks: List[str] = [],\n",
    "    ) -> None:\n",
    "        # calculate gradients\n",
    "        print( \"self.client_model_params_list[client_id] \",client_id,\"  \\n\")\n",
    "        # print(self.client_model_params_list[client_id])\n",
    "\n",
    "        hn_grads = torch.autograd.grad(\n",
    "            outputs=list(\n",
    "                filter(\n",
    "                    lambda param: param.requires_grad,\n",
    "                    self.client_model_params_list[client_id],\n",
    "                )\n",
    "            ),\n",
    "            inputs=self.hypernet.mlp_parameters()\n",
    "            + self.hypernet.fc_layer_parameters()\n",
    "            + self.hypernet.emd_parameters(),\n",
    "\n",
    "            grad_outputs=list(\n",
    "                map(\n",
    "                    lambda tup: tup[1],\n",
    "                    filter(\n",
    "                        lambda tup: tup[1].requires_grad\n",
    "                        and tup[0].split(\".\")[0] not in retain_blocks,\n",
    "                        diff.items(),\n",
    "                    ),\n",
    "                )\n",
    "            ),\n",
    "            allow_unused=True,\n",
    "        )\n",
    "        \n",
    "        mlp_grads = hn_grads[: len(self.hypernet.mlp_parameters())]\n",
    "        fc_grads = hn_grads[\n",
    "            len(self.hypernet.mlp_parameters()) : len(\n",
    "                self.hypernet.mlp_parameters() + self.hypernet.fc_layer_parameters()\n",
    "            )\n",
    "        ]\n",
    "        emd_grads = hn_grads[\n",
    "            len(self.hypernet.mlp_parameters() + self.hypernet.fc_layer_parameters()) :\n",
    "        ]\n",
    "\n",
    "        for param, grad in zip(self.hypernet.fc_layer_parameters(), fc_grads):\n",
    "            if grad is not None:\n",
    "                param.data -= self.args.hn_lr * grad\n",
    "\n",
    "        for param, grad in zip(self.hypernet.mlp_parameters(), mlp_grads):\n",
    "            param.data -= self.args.hn_lr * grad\n",
    "\n",
    "        for param, grad in zip(self.hypernet.emd_parameters(), emd_grads):\n",
    "            param.data -= self.args.hn_lr * grad\n",
    "\n",
    "        self.hypernet.save_hn()\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        super().run()\n",
    "        # clean out all HNs\n",
    "        self.hypernet.clean_models()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    server = pFedLAServer()\n",
    "    server.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb53d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd4f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9920615f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
