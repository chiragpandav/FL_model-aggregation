{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028d88d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import preprocessing\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from folktables import ACSDataSource, ACSEmployment,ACSIncome\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Dict, List, OrderedDict, Tuple\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, List, OrderedDict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87d866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 14 : input shape\n",
    "        self.layer1 = nn.Linear(14, 512)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(256, 60)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf7ee81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5a244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba37bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepNet(\n",
       "  (layer1): Linear(in_features=14, out_features=512, bias=True)\n",
       "  (act1): ReLU()\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (layer2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (act2): ReLU()\n",
       "  (layer3): Linear(in_features=256, out_features=60, bias=True)\n",
       "  (act3): ReLU()\n",
       "  (output): Linear(in_features=60, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_of_model='../../WW_WM_BW.pth'\n",
    "model = DeepNet()\n",
    "# model = Deep_wide_Net()\n",
    "model.load_state_dict(torch.load(name_of_model))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f74a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ffb0e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([152, 150])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "\n",
    "# Convert the state dictionary to a tensor\n",
    "tensor = torch.cat([value.flatten() for value in state_dict.values()])\n",
    "\n",
    "# Apply the topk function to the tensor\n",
    "_, top_i = torch.topk(tensor, k=2)\n",
    "\n",
    "top_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb570467",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = ['layer1.weight', 'layer1.bias', 'layer2.weight', 'layer2.bias', 'layer3.weight', 'layer3.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14ab5766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = []\n",
    "k=2\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        weights.append(torch.norm(param.data))\n",
    "\n",
    "# Convert the weights to a tensor\n",
    "weights_tensor = torch.stack(weights)\n",
    "\n",
    "# Find the indices of the top-k layers\n",
    "topk_indices = torch.topk(weights_tensor, k).indices\n",
    "\n",
    "topk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f6ad411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-2 layers:\n",
      "layer1.weight\n",
      "layer2.weight\n"
     ]
    }
   ],
   "source": [
    "# Get the top-k layer names\n",
    "layer_names = list(model.state_dict().keys())\n",
    "topk_layers = [layer_names[idx] for idx in topk_indices]\n",
    "\n",
    "print(\"Top-{} layers:\".format(k))\n",
    "for layer in topk_layers:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e82cae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['layer1.weight', 'layer2.weight']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6337f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aac5ca0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer1.weight': tensor([[-0.1236,  0.0936, -0.0858,  ..., -0.2022,  0.2071,  0.2143],\n",
      "        [-0.0173, -0.0282,  0.0289,  ..., -0.0473,  0.0984, -0.1676],\n",
      "        [ 0.2257, -0.0810, -0.0682,  ..., -0.0027, -0.1340, -0.0711],\n",
      "        ...,\n",
      "        [-0.0892, -0.2357,  0.0203,  ...,  0.2679, -0.0388, -0.2460],\n",
      "        [ 0.2170,  0.2365,  0.2169,  ..., -0.1546,  0.0313,  0.2786],\n",
      "        [-0.0207, -0.1458, -0.1536,  ...,  0.0709, -0.0735, -0.1318]]), 'layer1.bias': tensor([-0.0322,  0.0426, -0.0603,  0.0798,  0.1138,  0.1408,  0.0317,  0.2289,\n",
      "         0.1325, -0.1165, -0.1265,  0.2285, -0.0939,  0.1958,  0.2569,  0.1675,\n",
      "        -0.2616,  0.1116,  0.1401, -0.0685, -0.1891, -0.1634, -0.1235,  0.0884,\n",
      "        -0.2367,  0.2376, -0.0763, -0.0003,  0.1408, -0.0229,  0.1229, -0.1721,\n",
      "         0.0450, -0.2093,  0.0882, -0.2293, -0.2588,  0.1078, -0.0765,  0.0842,\n",
      "        -0.0346, -0.0134, -0.2554,  0.0831, -0.1555,  0.0582,  0.1946, -0.2204,\n",
      "         0.2441,  0.1677, -0.0461, -0.1893,  0.2440,  0.1668, -0.0349, -0.1506,\n",
      "        -0.1688,  0.2263,  0.0252, -0.1341,  0.1226,  0.0148, -0.2670, -0.2387,\n",
      "        -0.1646,  0.0938,  0.2166, -0.1961,  0.2032,  0.2083, -0.0538, -0.1734,\n",
      "        -0.0966,  0.1707,  0.2654, -0.1752,  0.1697,  0.0705,  0.0117, -0.1214,\n",
      "        -0.0225,  0.2655,  0.0326, -0.2243,  0.0494,  0.0262,  0.1043, -0.2016,\n",
      "        -0.1868,  0.2294,  0.1924, -0.0286,  0.1204,  0.0552,  0.1580,  0.2101,\n",
      "         0.0064,  0.1496,  0.0149, -0.2476,  0.2136, -0.2267, -0.1353, -0.2492,\n",
      "        -0.0971,  0.0536,  0.0151, -0.1011, -0.1989,  0.1396, -0.0581, -0.0835,\n",
      "        -0.0924,  0.0628,  0.0724, -0.1092,  0.1702, -0.1673, -0.1401, -0.0940,\n",
      "         0.1129,  0.2617, -0.1878,  0.0058, -0.0888,  0.1451, -0.1095, -0.1658,\n",
      "        -0.0124,  0.0627, -0.1805, -0.2139,  0.1665, -0.2399,  0.1778,  0.0173,\n",
      "         0.2214, -0.2617,  0.1081, -0.1959,  0.0678, -0.0350,  0.1809, -0.1450,\n",
      "         0.2169, -0.1114,  0.1557,  0.0868, -0.0219, -0.0083, -0.1890, -0.0412,\n",
      "        -0.2220,  0.1934,  0.2318, -0.0247, -0.1509, -0.0488, -0.0465,  0.0737,\n",
      "        -0.2061, -0.2349,  0.2161,  0.2344,  0.2316, -0.1617,  0.0875,  0.0097,\n",
      "        -0.0420,  0.0644, -0.1184,  0.0654, -0.1385,  0.0454,  0.2524,  0.1664,\n",
      "         0.1929, -0.1204,  0.2308, -0.1499,  0.0881,  0.0181,  0.2450, -0.1845,\n",
      "         0.1118, -0.0929,  0.0865,  0.2135, -0.0189, -0.0089,  0.1641,  0.0661,\n",
      "         0.1976, -0.1441,  0.0790,  0.2366,  0.0260, -0.0965, -0.1012, -0.1548,\n",
      "         0.1602, -0.0468, -0.2533,  0.1919,  0.1327,  0.1548, -0.1613,  0.1136,\n",
      "         0.1652, -0.1754,  0.1503, -0.0373, -0.2608,  0.1419, -0.2441,  0.2094,\n",
      "        -0.2494,  0.0223, -0.2322,  0.0267, -0.1226, -0.0819,  0.0992, -0.0039,\n",
      "         0.1870, -0.0248,  0.2247,  0.0420, -0.1182, -0.1147,  0.0921, -0.0033,\n",
      "         0.2048,  0.1877, -0.1732, -0.0861,  0.2154,  0.0813, -0.0482, -0.1471,\n",
      "         0.0729,  0.1672, -0.2414,  0.1081, -0.0761, -0.1497, -0.0425,  0.0763,\n",
      "         0.0946, -0.1367,  0.0379, -0.0749, -0.2108,  0.0960,  0.0245,  0.2244,\n",
      "         0.1602, -0.2316, -0.0304, -0.1809, -0.0896,  0.2048,  0.2831,  0.2165,\n",
      "        -0.2307, -0.1157, -0.0104,  0.0336,  0.1026,  0.2040,  0.0713, -0.2639,\n",
      "         0.0778,  0.0500,  0.1923,  0.1277,  0.1815, -0.2308,  0.1551, -0.2286,\n",
      "        -0.2252, -0.0512,  0.2110,  0.0355, -0.1942,  0.1886, -0.1394,  0.0509,\n",
      "         0.1202, -0.1873, -0.0009, -0.0283,  0.2121,  0.0804,  0.1885,  0.2071,\n",
      "        -0.1146,  0.1334,  0.0909,  0.1458,  0.0371, -0.1957,  0.2245, -0.2641,\n",
      "        -0.0151, -0.0276, -0.0836, -0.1191, -0.0810,  0.1114, -0.1471, -0.0285,\n",
      "        -0.0704,  0.1866,  0.2525,  0.0854,  0.1541, -0.1246, -0.2074,  0.1898,\n",
      "         0.2544,  0.1057,  0.0266, -0.2380,  0.2070,  0.0359,  0.0576,  0.1031,\n",
      "         0.1448, -0.2236, -0.0662,  0.1450, -0.1213, -0.1980, -0.0199,  0.1476,\n",
      "         0.1931, -0.1353,  0.1945, -0.0050,  0.0417, -0.0790, -0.1385,  0.1613,\n",
      "         0.1442,  0.0873, -0.1752,  0.1793,  0.2524, -0.2422,  0.1832,  0.2125,\n",
      "         0.0878, -0.2427, -0.1157, -0.1772,  0.1735,  0.0131,  0.0559, -0.0500,\n",
      "        -0.1939, -0.1549,  0.1681,  0.2576,  0.0758,  0.0899,  0.0342, -0.0172,\n",
      "        -0.1564, -0.2578,  0.1180, -0.2498, -0.0539, -0.1182, -0.0336, -0.1141,\n",
      "         0.0446, -0.1486,  0.1746,  0.0762, -0.1405,  0.0448, -0.2609,  0.0096,\n",
      "        -0.0782,  0.2262,  0.2258, -0.0248,  0.0938, -0.2441, -0.0882,  0.0792,\n",
      "        -0.1055, -0.1155,  0.0341,  0.1175,  0.0215, -0.0857, -0.1695, -0.1577,\n",
      "        -0.1969,  0.1435, -0.2649,  0.1795, -0.1148,  0.2169,  0.0949, -0.2064,\n",
      "         0.0621, -0.1601,  0.2444, -0.0065,  0.1467,  0.0220,  0.0661,  0.1205,\n",
      "         0.0683, -0.1975,  0.0651,  0.2035,  0.1648, -0.2009, -0.2092, -0.2035,\n",
      "         0.0442,  0.2364, -0.2322, -0.1091, -0.1303, -0.0804, -0.1521,  0.0437,\n",
      "        -0.0293, -0.0676,  0.2063, -0.2415, -0.0775,  0.0658,  0.1934,  0.1241,\n",
      "         0.2322,  0.0168, -0.0104,  0.0872,  0.0879, -0.1017, -0.0718, -0.1307,\n",
      "         0.2594, -0.2004,  0.1750,  0.0769,  0.2043,  0.0724, -0.2261,  0.0856,\n",
      "        -0.1057, -0.2325,  0.1403,  0.0057,  0.1419,  0.0259,  0.1827, -0.0589,\n",
      "         0.0746, -0.0092, -0.0078, -0.1230,  0.1779,  0.1347, -0.0604,  0.1400,\n",
      "        -0.0195,  0.1195, -0.2178,  0.2222,  0.0282, -0.0979,  0.0141, -0.0208,\n",
      "         0.0783,  0.0834,  0.1010,  0.1194,  0.0306,  0.1227, -0.0238, -0.0877,\n",
      "        -0.1093,  0.0120,  0.2259,  0.1914, -0.1404,  0.1296, -0.1403,  0.1636,\n",
      "        -0.2438,  0.1688,  0.2359, -0.0039,  0.1959, -0.2448,  0.1091, -0.0898,\n",
      "        -0.0822, -0.1436, -0.0462,  0.1166, -0.1012,  0.2074,  0.0394,  0.0921]), 'layer2.weight': tensor([[ 4.1278e-02, -6.7849e-03, -4.5960e-02,  ...,  6.2805e-05,\n",
      "         -2.6595e-02, -1.8976e-02],\n",
      "        [ 1.7838e-03, -2.4043e-02,  7.7809e-04,  ..., -7.5658e-02,\n",
      "          5.3687e-02, -8.9672e-03],\n",
      "        [ 1.0132e-02, -3.3095e-02, -2.9559e-02,  ..., -4.8992e-02,\n",
      "         -7.1659e-03,  2.1242e-02],\n",
      "        ...,\n",
      "        [-1.7378e-02, -8.9730e-03, -1.6638e-02,  ..., -2.5265e-02,\n",
      "          2.3593e-02,  1.8103e-02],\n",
      "        [ 3.5799e-02,  8.1830e-02,  2.2687e-03,  ...,  6.2357e-02,\n",
      "          1.8346e-02,  4.6302e-02],\n",
      "        [ 8.7989e-02,  4.6054e-02,  4.5712e-02,  ...,  2.1118e-02,\n",
      "         -5.3984e-02,  1.1680e-01]]), 'layer2.bias': tensor([ 0.0384,  0.0524,  0.0343, -0.0506,  0.0003, -0.0071, -0.0294, -0.0185,\n",
      "         0.0118,  0.0057, -0.0228, -0.0090,  0.0057,  0.0404, -0.0217,  0.0036,\n",
      "        -0.0181,  0.0074, -0.0317, -0.0342,  0.0071, -0.0288, -0.0408,  0.0442,\n",
      "         0.0342, -0.0332, -0.0362,  0.0176, -0.0375,  0.0318,  0.0404,  0.0458,\n",
      "         0.0078,  0.0040,  0.0200, -0.0366,  0.0267, -0.0022,  0.0354, -0.0474,\n",
      "        -0.0166, -0.0006, -0.0007,  0.0400, -0.0372, -0.0399, -0.0033, -0.0008,\n",
      "         0.0100, -0.0250,  0.0383,  0.0199,  0.0128,  0.0305,  0.0343, -0.0221,\n",
      "         0.0687,  0.0022,  0.0105,  0.0151,  0.0002, -0.0363,  0.0017,  0.0028,\n",
      "         0.0230,  0.0226, -0.0056, -0.0060, -0.0110, -0.0195, -0.0237,  0.0310,\n",
      "         0.0229,  0.0102, -0.0455,  0.0325,  0.0163,  0.0451,  0.0147, -0.0333,\n",
      "        -0.0341, -0.0206, -0.0069,  0.0169,  0.0133, -0.0024, -0.0069, -0.0051,\n",
      "        -0.0018, -0.0373,  0.0120,  0.0156,  0.0120, -0.0171, -0.0177,  0.0141,\n",
      "        -0.0035, -0.0330, -0.0130, -0.0467, -0.0018,  0.0134, -0.0195, -0.0312,\n",
      "         0.0419,  0.0307,  0.0108, -0.0144,  0.0031,  0.0107, -0.0223,  0.0073,\n",
      "         0.0363, -0.0120, -0.0274, -0.0069,  0.0219,  0.0114, -0.0360,  0.0247,\n",
      "        -0.0016,  0.0057, -0.0445, -0.0320, -0.0266,  0.0085,  0.0428,  0.0379,\n",
      "        -0.0180,  0.0529,  0.0405, -0.0164,  0.0095, -0.0098,  0.0112,  0.0060,\n",
      "         0.0351, -0.0354,  0.0060, -0.0238, -0.0246,  0.0400,  0.0024,  0.0903,\n",
      "         0.0025,  0.0171, -0.0017, -0.0331, -0.0256,  0.0092, -0.0064, -0.0089,\n",
      "        -0.0344,  0.0494, -0.0250, -0.0384, -0.0258, -0.0120, -0.0347,  0.0508,\n",
      "        -0.0008, -0.0334, -0.0415,  0.0257,  0.0609,  0.0077, -0.0350, -0.0118,\n",
      "        -0.0241,  0.0029,  0.0233, -0.0546, -0.0086, -0.0486, -0.0328,  0.0228,\n",
      "        -0.0439, -0.0425, -0.0320,  0.0123, -0.0224,  0.0212, -0.0295,  0.0227,\n",
      "         0.0373,  0.0509, -0.0486,  0.0102,  0.0256,  0.0283, -0.0049,  0.0052,\n",
      "        -0.0157,  0.0483,  0.0189,  0.0068, -0.0114,  0.0213, -0.0225,  0.0361,\n",
      "        -0.0154, -0.0002,  0.0180, -0.0240,  0.0241,  0.0006,  0.0318, -0.0221,\n",
      "        -0.0064, -0.0498,  0.0070,  0.0372, -0.0281, -0.0068, -0.0252, -0.0090,\n",
      "         0.0113,  0.0234, -0.0158,  0.0324, -0.0249, -0.0172, -0.0205, -0.0013,\n",
      "         0.0095, -0.0057, -0.0430, -0.0238, -0.0108, -0.0365, -0.0168,  0.0380,\n",
      "        -0.0142,  0.0281, -0.0181,  0.0036,  0.0233,  0.0110,  0.0047,  0.0148,\n",
      "        -0.0202,  0.0722, -0.0015, -0.0202,  0.0105, -0.0281, -0.0308, -0.0169,\n",
      "         0.0263,  0.0244, -0.0248,  0.0092,  0.0101,  0.0149,  0.0076, -0.0055]), 'layer3.weight': tensor([[ 5.4633e-02,  8.1261e-03, -3.0832e-02,  ...,  5.4763e-02,\n",
      "         -3.9070e-02, -4.7520e-02],\n",
      "        [ 7.2382e-02, -4.7307e-02, -2.3797e-02,  ...,  3.9055e-03,\n",
      "         -1.5235e-02, -1.7146e-02],\n",
      "        [-1.7924e-02,  4.6757e-04, -4.0349e-02,  ...,  5.7521e-02,\n",
      "          9.0531e-02, -2.5051e-03],\n",
      "        ...,\n",
      "        [ 1.4053e-02, -4.3309e-02,  3.2839e-05,  ..., -2.4532e-02,\n",
      "         -2.0915e-02,  6.3625e-02],\n",
      "        [-5.1204e-03,  4.1185e-02,  1.7763e-02,  ...,  2.1898e-03,\n",
      "         -7.2193e-02,  1.9890e-02],\n",
      "        [-5.4119e-02,  2.7216e-02, -4.5889e-02,  ...,  1.9468e-03,\n",
      "          1.6108e-02,  3.3428e-02]]), 'layer3.bias': tensor([ 0.0218,  0.0122, -0.0090,  0.0192,  0.0073, -0.0146, -0.0172, -0.0161,\n",
      "         0.0034, -0.0175, -0.0405,  0.0299, -0.0603, -0.0260,  0.0148, -0.0506,\n",
      "         0.0101,  0.0233,  0.0611,  0.0053,  0.0120,  0.0282, -0.0101, -0.0045,\n",
      "         0.0508,  0.0130, -0.0003, -0.0131, -0.0401,  0.0463, -0.0210,  0.0293,\n",
      "        -0.0256,  0.0346,  0.0168, -0.0242, -0.0049, -0.0485,  0.0420,  0.0150,\n",
      "         0.0399, -0.0609, -0.0609,  0.0046,  0.0419, -0.0323, -0.0594,  0.0583,\n",
      "        -0.0254, -0.0278,  0.0016, -0.0256, -0.0305,  0.0366, -0.0453,  0.0189,\n",
      "        -0.0292,  0.0077,  0.0030, -0.0285]), 'output.weight': tensor([[ 0.0554, -0.0423, -0.1156,  0.0730, -0.0532,  0.0089, -0.0901, -0.1038,\n",
      "         -0.1392,  0.0981,  0.0516,  0.1187, -0.0476, -0.1402,  0.0835,  0.0997,\n",
      "         -0.1758,  0.1161,  0.1730,  0.0904, -0.0909,  0.0716, -0.0481, -0.0731,\n",
      "         -0.1037, -0.0566, -0.0789,  0.0637,  0.0840, -0.0526,  0.0498,  0.0710,\n",
      "         -0.0173,  0.0521, -0.0801,  0.1083, -0.1271,  0.0110,  0.1764, -0.1537,\n",
      "         -0.1162, -0.0945, -0.1474,  0.1394,  0.0944,  0.0190,  0.0964, -0.1173,\n",
      "         -0.1240,  0.1212,  0.1095, -0.0452,  0.0376, -0.1081, -0.0215, -0.1443,\n",
      "          0.0689, -0.1461,  0.1446, -0.0005]]), 'output.bias': tensor([-0.0053])}\n"
     ]
    }
   ],
   "source": [
    "alpha = {}\n",
    "for name, _ in model.named_parameters():\n",
    "    alpha[name] = model.state_dict()[name]\n",
    "\n",
    "print(alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ae736d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1236,  0.0936, -0.0858,  ..., -0.2022,  0.2071,  0.2143],\n",
       "        [-0.0173, -0.0282,  0.0289,  ..., -0.0473,  0.0984, -0.1676],\n",
       "        [ 0.2257, -0.0810, -0.0682,  ..., -0.0027, -0.1340, -0.0711],\n",
       "        ...,\n",
       "        [-0.0892, -0.2357,  0.0203,  ...,  0.2679, -0.0388, -0.2460],\n",
       "        [ 0.2170,  0.2365,  0.2169,  ..., -0.1546,  0.0313,  0.2786],\n",
       "        [-0.0207, -0.1458, -0.1536,  ...,  0.0709, -0.0735, -0.1318]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha['layer1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42d13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a06087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params_name = [name for name in model.state_dict().keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1adf70f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['layer1.weight',\n",
       " 'layer1.bias',\n",
       " 'layer2.weight',\n",
       " 'layer2.bias',\n",
       " 'layer3.weight',\n",
       " 'layer3.bias',\n",
       " 'output.weight',\n",
       " 'output.bias']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_params_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cce6d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params_name = [name for name, param in model.state_dict(keep_vars=True).items()\n",
    "            if param.requires_grad ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69b3f86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['layer1.weight',\n",
       " 'layer1.bias',\n",
       " 'layer2.weight',\n",
       " 'layer2.bias',\n",
       " 'layer3.weight',\n",
       " 'layer3.bias',\n",
       " 'output.weight',\n",
       " 'output.bias']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_params_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a65af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=model.state_dict()['layer1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7990963a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 14])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e652714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1236,  0.0936, -0.0858,  ..., -0.2022,  0.2071,  0.2143],\n",
       "        [-0.0173, -0.0282,  0.0289,  ..., -0.0473,  0.0984, -0.1676],\n",
       "        [ 0.2257, -0.0810, -0.0682,  ..., -0.0027, -0.1340, -0.0711],\n",
       "        ...,\n",
       "        [-0.0892, -0.2357,  0.0203,  ...,  0.2679, -0.0388, -0.2460],\n",
       "        [ 0.2170,  0.2365,  0.2169,  ..., -0.1546,  0.0313,  0.2786],\n",
       "        [-0.0207, -0.1458, -0.1536,  ...,  0.0709, -0.0735, -0.1318]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['layer1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66a8308a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_values([tensor([[-0.1236,  0.0936, -0.0858,  ..., -0.2022,  0.2071,  0.2143],\n",
       "        [-0.0173, -0.0282,  0.0289,  ..., -0.0473,  0.0984, -0.1676],\n",
       "        [ 0.2257, -0.0810, -0.0682,  ..., -0.0027, -0.1340, -0.0711],\n",
       "        ...,\n",
       "        [-0.0892, -0.2357,  0.0203,  ...,  0.2679, -0.0388, -0.2460],\n",
       "        [ 0.2170,  0.2365,  0.2169,  ..., -0.1546,  0.0313,  0.2786],\n",
       "        [-0.0207, -0.1458, -0.1536,  ...,  0.0709, -0.0735, -0.1318]]), tensor([-0.0322,  0.0426, -0.0603,  0.0798,  0.1138,  0.1408,  0.0317,  0.2289,\n",
       "         0.1325, -0.1165, -0.1265,  0.2285, -0.0939,  0.1958,  0.2569,  0.1675,\n",
       "        -0.2616,  0.1116,  0.1401, -0.0685, -0.1891, -0.1634, -0.1235,  0.0884,\n",
       "        -0.2367,  0.2376, -0.0763, -0.0003,  0.1408, -0.0229,  0.1229, -0.1721,\n",
       "         0.0450, -0.2093,  0.0882, -0.2293, -0.2588,  0.1078, -0.0765,  0.0842,\n",
       "        -0.0346, -0.0134, -0.2554,  0.0831, -0.1555,  0.0582,  0.1946, -0.2204,\n",
       "         0.2441,  0.1677, -0.0461, -0.1893,  0.2440,  0.1668, -0.0349, -0.1506,\n",
       "        -0.1688,  0.2263,  0.0252, -0.1341,  0.1226,  0.0148, -0.2670, -0.2387,\n",
       "        -0.1646,  0.0938,  0.2166, -0.1961,  0.2032,  0.2083, -0.0538, -0.1734,\n",
       "        -0.0966,  0.1707,  0.2654, -0.1752,  0.1697,  0.0705,  0.0117, -0.1214,\n",
       "        -0.0225,  0.2655,  0.0326, -0.2243,  0.0494,  0.0262,  0.1043, -0.2016,\n",
       "        -0.1868,  0.2294,  0.1924, -0.0286,  0.1204,  0.0552,  0.1580,  0.2101,\n",
       "         0.0064,  0.1496,  0.0149, -0.2476,  0.2136, -0.2267, -0.1353, -0.2492,\n",
       "        -0.0971,  0.0536,  0.0151, -0.1011, -0.1989,  0.1396, -0.0581, -0.0835,\n",
       "        -0.0924,  0.0628,  0.0724, -0.1092,  0.1702, -0.1673, -0.1401, -0.0940,\n",
       "         0.1129,  0.2617, -0.1878,  0.0058, -0.0888,  0.1451, -0.1095, -0.1658,\n",
       "        -0.0124,  0.0627, -0.1805, -0.2139,  0.1665, -0.2399,  0.1778,  0.0173,\n",
       "         0.2214, -0.2617,  0.1081, -0.1959,  0.0678, -0.0350,  0.1809, -0.1450,\n",
       "         0.2169, -0.1114,  0.1557,  0.0868, -0.0219, -0.0083, -0.1890, -0.0412,\n",
       "        -0.2220,  0.1934,  0.2318, -0.0247, -0.1509, -0.0488, -0.0465,  0.0737,\n",
       "        -0.2061, -0.2349,  0.2161,  0.2344,  0.2316, -0.1617,  0.0875,  0.0097,\n",
       "        -0.0420,  0.0644, -0.1184,  0.0654, -0.1385,  0.0454,  0.2524,  0.1664,\n",
       "         0.1929, -0.1204,  0.2308, -0.1499,  0.0881,  0.0181,  0.2450, -0.1845,\n",
       "         0.1118, -0.0929,  0.0865,  0.2135, -0.0189, -0.0089,  0.1641,  0.0661,\n",
       "         0.1976, -0.1441,  0.0790,  0.2366,  0.0260, -0.0965, -0.1012, -0.1548,\n",
       "         0.1602, -0.0468, -0.2533,  0.1919,  0.1327,  0.1548, -0.1613,  0.1136,\n",
       "         0.1652, -0.1754,  0.1503, -0.0373, -0.2608,  0.1419, -0.2441,  0.2094,\n",
       "        -0.2494,  0.0223, -0.2322,  0.0267, -0.1226, -0.0819,  0.0992, -0.0039,\n",
       "         0.1870, -0.0248,  0.2247,  0.0420, -0.1182, -0.1147,  0.0921, -0.0033,\n",
       "         0.2048,  0.1877, -0.1732, -0.0861,  0.2154,  0.0813, -0.0482, -0.1471,\n",
       "         0.0729,  0.1672, -0.2414,  0.1081, -0.0761, -0.1497, -0.0425,  0.0763,\n",
       "         0.0946, -0.1367,  0.0379, -0.0749, -0.2108,  0.0960,  0.0245,  0.2244,\n",
       "         0.1602, -0.2316, -0.0304, -0.1809, -0.0896,  0.2048,  0.2831,  0.2165,\n",
       "        -0.2307, -0.1157, -0.0104,  0.0336,  0.1026,  0.2040,  0.0713, -0.2639,\n",
       "         0.0778,  0.0500,  0.1923,  0.1277,  0.1815, -0.2308,  0.1551, -0.2286,\n",
       "        -0.2252, -0.0512,  0.2110,  0.0355, -0.1942,  0.1886, -0.1394,  0.0509,\n",
       "         0.1202, -0.1873, -0.0009, -0.0283,  0.2121,  0.0804,  0.1885,  0.2071,\n",
       "        -0.1146,  0.1334,  0.0909,  0.1458,  0.0371, -0.1957,  0.2245, -0.2641,\n",
       "        -0.0151, -0.0276, -0.0836, -0.1191, -0.0810,  0.1114, -0.1471, -0.0285,\n",
       "        -0.0704,  0.1866,  0.2525,  0.0854,  0.1541, -0.1246, -0.2074,  0.1898,\n",
       "         0.2544,  0.1057,  0.0266, -0.2380,  0.2070,  0.0359,  0.0576,  0.1031,\n",
       "         0.1448, -0.2236, -0.0662,  0.1450, -0.1213, -0.1980, -0.0199,  0.1476,\n",
       "         0.1931, -0.1353,  0.1945, -0.0050,  0.0417, -0.0790, -0.1385,  0.1613,\n",
       "         0.1442,  0.0873, -0.1752,  0.1793,  0.2524, -0.2422,  0.1832,  0.2125,\n",
       "         0.0878, -0.2427, -0.1157, -0.1772,  0.1735,  0.0131,  0.0559, -0.0500,\n",
       "        -0.1939, -0.1549,  0.1681,  0.2576,  0.0758,  0.0899,  0.0342, -0.0172,\n",
       "        -0.1564, -0.2578,  0.1180, -0.2498, -0.0539, -0.1182, -0.0336, -0.1141,\n",
       "         0.0446, -0.1486,  0.1746,  0.0762, -0.1405,  0.0448, -0.2609,  0.0096,\n",
       "        -0.0782,  0.2262,  0.2258, -0.0248,  0.0938, -0.2441, -0.0882,  0.0792,\n",
       "        -0.1055, -0.1155,  0.0341,  0.1175,  0.0215, -0.0857, -0.1695, -0.1577,\n",
       "        -0.1969,  0.1435, -0.2649,  0.1795, -0.1148,  0.2169,  0.0949, -0.2064,\n",
       "         0.0621, -0.1601,  0.2444, -0.0065,  0.1467,  0.0220,  0.0661,  0.1205,\n",
       "         0.0683, -0.1975,  0.0651,  0.2035,  0.1648, -0.2009, -0.2092, -0.2035,\n",
       "         0.0442,  0.2364, -0.2322, -0.1091, -0.1303, -0.0804, -0.1521,  0.0437,\n",
       "        -0.0293, -0.0676,  0.2063, -0.2415, -0.0775,  0.0658,  0.1934,  0.1241,\n",
       "         0.2322,  0.0168, -0.0104,  0.0872,  0.0879, -0.1017, -0.0718, -0.1307,\n",
       "         0.2594, -0.2004,  0.1750,  0.0769,  0.2043,  0.0724, -0.2261,  0.0856,\n",
       "        -0.1057, -0.2325,  0.1403,  0.0057,  0.1419,  0.0259,  0.1827, -0.0589,\n",
       "         0.0746, -0.0092, -0.0078, -0.1230,  0.1779,  0.1347, -0.0604,  0.1400,\n",
       "        -0.0195,  0.1195, -0.2178,  0.2222,  0.0282, -0.0979,  0.0141, -0.0208,\n",
       "         0.0783,  0.0834,  0.1010,  0.1194,  0.0306,  0.1227, -0.0238, -0.0877,\n",
       "        -0.1093,  0.0120,  0.2259,  0.1914, -0.1404,  0.1296, -0.1403,  0.1636,\n",
       "        -0.2438,  0.1688,  0.2359, -0.0039,  0.1959, -0.2448,  0.1091, -0.0898,\n",
       "        -0.0822, -0.1436, -0.0462,  0.1166, -0.1012,  0.2074,  0.0394,  0.0921]), tensor([[ 4.1278e-02, -6.7849e-03, -4.5960e-02,  ...,  6.2805e-05,\n",
       "         -2.6595e-02, -1.8976e-02],\n",
       "        [ 1.7838e-03, -2.4043e-02,  7.7809e-04,  ..., -7.5658e-02,\n",
       "          5.3687e-02, -8.9672e-03],\n",
       "        [ 1.0132e-02, -3.3095e-02, -2.9559e-02,  ..., -4.8992e-02,\n",
       "         -7.1659e-03,  2.1242e-02],\n",
       "        ...,\n",
       "        [-1.7378e-02, -8.9730e-03, -1.6638e-02,  ..., -2.5265e-02,\n",
       "          2.3593e-02,  1.8103e-02],\n",
       "        [ 3.5799e-02,  8.1830e-02,  2.2687e-03,  ...,  6.2357e-02,\n",
       "          1.8346e-02,  4.6302e-02],\n",
       "        [ 8.7989e-02,  4.6054e-02,  4.5712e-02,  ...,  2.1118e-02,\n",
       "         -5.3984e-02,  1.1680e-01]]), tensor([ 0.0384,  0.0524,  0.0343, -0.0506,  0.0003, -0.0071, -0.0294, -0.0185,\n",
       "         0.0118,  0.0057, -0.0228, -0.0090,  0.0057,  0.0404, -0.0217,  0.0036,\n",
       "        -0.0181,  0.0074, -0.0317, -0.0342,  0.0071, -0.0288, -0.0408,  0.0442,\n",
       "         0.0342, -0.0332, -0.0362,  0.0176, -0.0375,  0.0318,  0.0404,  0.0458,\n",
       "         0.0078,  0.0040,  0.0200, -0.0366,  0.0267, -0.0022,  0.0354, -0.0474,\n",
       "        -0.0166, -0.0006, -0.0007,  0.0400, -0.0372, -0.0399, -0.0033, -0.0008,\n",
       "         0.0100, -0.0250,  0.0383,  0.0199,  0.0128,  0.0305,  0.0343, -0.0221,\n",
       "         0.0687,  0.0022,  0.0105,  0.0151,  0.0002, -0.0363,  0.0017,  0.0028,\n",
       "         0.0230,  0.0226, -0.0056, -0.0060, -0.0110, -0.0195, -0.0237,  0.0310,\n",
       "         0.0229,  0.0102, -0.0455,  0.0325,  0.0163,  0.0451,  0.0147, -0.0333,\n",
       "        -0.0341, -0.0206, -0.0069,  0.0169,  0.0133, -0.0024, -0.0069, -0.0051,\n",
       "        -0.0018, -0.0373,  0.0120,  0.0156,  0.0120, -0.0171, -0.0177,  0.0141,\n",
       "        -0.0035, -0.0330, -0.0130, -0.0467, -0.0018,  0.0134, -0.0195, -0.0312,\n",
       "         0.0419,  0.0307,  0.0108, -0.0144,  0.0031,  0.0107, -0.0223,  0.0073,\n",
       "         0.0363, -0.0120, -0.0274, -0.0069,  0.0219,  0.0114, -0.0360,  0.0247,\n",
       "        -0.0016,  0.0057, -0.0445, -0.0320, -0.0266,  0.0085,  0.0428,  0.0379,\n",
       "        -0.0180,  0.0529,  0.0405, -0.0164,  0.0095, -0.0098,  0.0112,  0.0060,\n",
       "         0.0351, -0.0354,  0.0060, -0.0238, -0.0246,  0.0400,  0.0024,  0.0903,\n",
       "         0.0025,  0.0171, -0.0017, -0.0331, -0.0256,  0.0092, -0.0064, -0.0089,\n",
       "        -0.0344,  0.0494, -0.0250, -0.0384, -0.0258, -0.0120, -0.0347,  0.0508,\n",
       "        -0.0008, -0.0334, -0.0415,  0.0257,  0.0609,  0.0077, -0.0350, -0.0118,\n",
       "        -0.0241,  0.0029,  0.0233, -0.0546, -0.0086, -0.0486, -0.0328,  0.0228,\n",
       "        -0.0439, -0.0425, -0.0320,  0.0123, -0.0224,  0.0212, -0.0295,  0.0227,\n",
       "         0.0373,  0.0509, -0.0486,  0.0102,  0.0256,  0.0283, -0.0049,  0.0052,\n",
       "        -0.0157,  0.0483,  0.0189,  0.0068, -0.0114,  0.0213, -0.0225,  0.0361,\n",
       "        -0.0154, -0.0002,  0.0180, -0.0240,  0.0241,  0.0006,  0.0318, -0.0221,\n",
       "        -0.0064, -0.0498,  0.0070,  0.0372, -0.0281, -0.0068, -0.0252, -0.0090,\n",
       "         0.0113,  0.0234, -0.0158,  0.0324, -0.0249, -0.0172, -0.0205, -0.0013,\n",
       "         0.0095, -0.0057, -0.0430, -0.0238, -0.0108, -0.0365, -0.0168,  0.0380,\n",
       "        -0.0142,  0.0281, -0.0181,  0.0036,  0.0233,  0.0110,  0.0047,  0.0148,\n",
       "        -0.0202,  0.0722, -0.0015, -0.0202,  0.0105, -0.0281, -0.0308, -0.0169,\n",
       "         0.0263,  0.0244, -0.0248,  0.0092,  0.0101,  0.0149,  0.0076, -0.0055]), tensor([[ 5.4633e-02,  8.1261e-03, -3.0832e-02,  ...,  5.4763e-02,\n",
       "         -3.9070e-02, -4.7520e-02],\n",
       "        [ 7.2382e-02, -4.7307e-02, -2.3797e-02,  ...,  3.9055e-03,\n",
       "         -1.5235e-02, -1.7146e-02],\n",
       "        [-1.7924e-02,  4.6757e-04, -4.0349e-02,  ...,  5.7521e-02,\n",
       "          9.0531e-02, -2.5051e-03],\n",
       "        ...,\n",
       "        [ 1.4053e-02, -4.3309e-02,  3.2839e-05,  ..., -2.4532e-02,\n",
       "         -2.0915e-02,  6.3625e-02],\n",
       "        [-5.1204e-03,  4.1185e-02,  1.7763e-02,  ...,  2.1898e-03,\n",
       "         -7.2193e-02,  1.9890e-02],\n",
       "        [-5.4119e-02,  2.7216e-02, -4.5889e-02,  ...,  1.9468e-03,\n",
       "          1.6108e-02,  3.3428e-02]]), tensor([ 0.0218,  0.0122, -0.0090,  0.0192,  0.0073, -0.0146, -0.0172, -0.0161,\n",
       "         0.0034, -0.0175, -0.0405,  0.0299, -0.0603, -0.0260,  0.0148, -0.0506,\n",
       "         0.0101,  0.0233,  0.0611,  0.0053,  0.0120,  0.0282, -0.0101, -0.0045,\n",
       "         0.0508,  0.0130, -0.0003, -0.0131, -0.0401,  0.0463, -0.0210,  0.0293,\n",
       "        -0.0256,  0.0346,  0.0168, -0.0242, -0.0049, -0.0485,  0.0420,  0.0150,\n",
       "         0.0399, -0.0609, -0.0609,  0.0046,  0.0419, -0.0323, -0.0594,  0.0583,\n",
       "        -0.0254, -0.0278,  0.0016, -0.0256, -0.0305,  0.0366, -0.0453,  0.0189,\n",
       "        -0.0292,  0.0077,  0.0030, -0.0285]), tensor([[ 0.0554, -0.0423, -0.1156,  0.0730, -0.0532,  0.0089, -0.0901, -0.1038,\n",
       "         -0.1392,  0.0981,  0.0516,  0.1187, -0.0476, -0.1402,  0.0835,  0.0997,\n",
       "         -0.1758,  0.1161,  0.1730,  0.0904, -0.0909,  0.0716, -0.0481, -0.0731,\n",
       "         -0.1037, -0.0566, -0.0789,  0.0637,  0.0840, -0.0526,  0.0498,  0.0710,\n",
       "         -0.0173,  0.0521, -0.0801,  0.1083, -0.1271,  0.0110,  0.1764, -0.1537,\n",
       "         -0.1162, -0.0945, -0.1474,  0.1394,  0.0944,  0.0190,  0.0964, -0.1173,\n",
       "         -0.1240,  0.1212,  0.1095, -0.0452,  0.0376, -0.1081, -0.0215, -0.1443,\n",
       "          0.0689, -0.1461,  0.1446, -0.0005]]), tensor([-0.0053])])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13f0aaa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1236],\n",
       "         [ 0.0936],\n",
       "         [-0.0858],\n",
       "         ...,\n",
       "         [-0.2022],\n",
       "         [ 0.2071],\n",
       "         [ 0.2143]],\n",
       "\n",
       "        [[-0.0173],\n",
       "         [-0.0282],\n",
       "         [ 0.0289],\n",
       "         ...,\n",
       "         [-0.0473],\n",
       "         [ 0.0984],\n",
       "         [-0.1676]],\n",
       "\n",
       "        [[ 0.2257],\n",
       "         [-0.0810],\n",
       "         [-0.0682],\n",
       "         ...,\n",
       "         [-0.0027],\n",
       "         [-0.1340],\n",
       "         [-0.0711]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0892],\n",
       "         [-0.2357],\n",
       "         [ 0.0203],\n",
       "         ...,\n",
       "         [ 0.2679],\n",
       "         [-0.0388],\n",
       "         [-0.2460]],\n",
       "\n",
       "        [[ 0.2170],\n",
       "         [ 0.2365],\n",
       "         [ 0.2169],\n",
       "         ...,\n",
       "         [-0.1546],\n",
       "         [ 0.0313],\n",
       "         [ 0.2786]],\n",
       "\n",
       "        [[-0.0207],\n",
       "         [-0.1458],\n",
       "         [-0.1536],\n",
       "         ...,\n",
       "         [ 0.0709],\n",
       "         [-0.0735],\n",
       "         [-0.1318]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(( model.state_dict()['layer1.weight'],), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f1c4465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 14])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a/a.sum()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2811a436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(108.5115)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a/(a.sum()*torch.stack(( model.state_dict()['layer1.weight'],))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f67c2e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1236,  0.0936, -0.0858,  ..., -0.2022,  0.2071,  0.2143],\n",
       "         [-0.0173, -0.0282,  0.0289,  ..., -0.0473,  0.0984, -0.1676],\n",
       "         [ 0.2257, -0.0810, -0.0682,  ..., -0.0027, -0.1340, -0.0711],\n",
       "         ...,\n",
       "         [-0.0892, -0.2357,  0.0203,  ...,  0.2679, -0.0388, -0.2460],\n",
       "         [ 0.2170,  0.2365,  0.2169,  ..., -0.1546,  0.0313,  0.2786],\n",
       "         [-0.0207, -0.1458, -0.1536,  ...,  0.0709, -0.0735, -0.1318]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0322,  0.0426, -0.0603,  0.0798,  0.1138,  0.1408,  0.0317,  0.2289,\n",
       "          0.1325, -0.1165, -0.1265,  0.2285, -0.0939,  0.1958,  0.2569,  0.1675,\n",
       "         -0.2616,  0.1116,  0.1401, -0.0685, -0.1891, -0.1634, -0.1235,  0.0884,\n",
       "         -0.2367,  0.2376, -0.0763, -0.0003,  0.1408, -0.0229,  0.1229, -0.1721,\n",
       "          0.0450, -0.2093,  0.0882, -0.2293, -0.2588,  0.1078, -0.0765,  0.0842,\n",
       "         -0.0346, -0.0134, -0.2554,  0.0831, -0.1555,  0.0582,  0.1946, -0.2204,\n",
       "          0.2441,  0.1677, -0.0461, -0.1893,  0.2440,  0.1668, -0.0349, -0.1506,\n",
       "         -0.1688,  0.2263,  0.0252, -0.1341,  0.1226,  0.0148, -0.2670, -0.2387,\n",
       "         -0.1646,  0.0938,  0.2166, -0.1961,  0.2032,  0.2083, -0.0538, -0.1734,\n",
       "         -0.0966,  0.1707,  0.2654, -0.1752,  0.1697,  0.0705,  0.0117, -0.1214,\n",
       "         -0.0225,  0.2655,  0.0326, -0.2243,  0.0494,  0.0262,  0.1043, -0.2016,\n",
       "         -0.1868,  0.2294,  0.1924, -0.0286,  0.1204,  0.0552,  0.1580,  0.2101,\n",
       "          0.0064,  0.1496,  0.0149, -0.2476,  0.2136, -0.2267, -0.1353, -0.2492,\n",
       "         -0.0971,  0.0536,  0.0151, -0.1011, -0.1989,  0.1396, -0.0581, -0.0835,\n",
       "         -0.0924,  0.0628,  0.0724, -0.1092,  0.1702, -0.1673, -0.1401, -0.0940,\n",
       "          0.1129,  0.2617, -0.1878,  0.0058, -0.0888,  0.1451, -0.1095, -0.1658,\n",
       "         -0.0124,  0.0627, -0.1805, -0.2139,  0.1665, -0.2399,  0.1778,  0.0173,\n",
       "          0.2214, -0.2617,  0.1081, -0.1959,  0.0678, -0.0350,  0.1809, -0.1450,\n",
       "          0.2169, -0.1114,  0.1557,  0.0868, -0.0219, -0.0083, -0.1890, -0.0412,\n",
       "         -0.2220,  0.1934,  0.2318, -0.0247, -0.1509, -0.0488, -0.0465,  0.0737,\n",
       "         -0.2061, -0.2349,  0.2161,  0.2344,  0.2316, -0.1617,  0.0875,  0.0097,\n",
       "         -0.0420,  0.0644, -0.1184,  0.0654, -0.1385,  0.0454,  0.2524,  0.1664,\n",
       "          0.1929, -0.1204,  0.2308, -0.1499,  0.0881,  0.0181,  0.2450, -0.1845,\n",
       "          0.1118, -0.0929,  0.0865,  0.2135, -0.0189, -0.0089,  0.1641,  0.0661,\n",
       "          0.1976, -0.1441,  0.0790,  0.2366,  0.0260, -0.0965, -0.1012, -0.1548,\n",
       "          0.1602, -0.0468, -0.2533,  0.1919,  0.1327,  0.1548, -0.1613,  0.1136,\n",
       "          0.1652, -0.1754,  0.1503, -0.0373, -0.2608,  0.1419, -0.2441,  0.2094,\n",
       "         -0.2494,  0.0223, -0.2322,  0.0267, -0.1226, -0.0819,  0.0992, -0.0039,\n",
       "          0.1870, -0.0248,  0.2247,  0.0420, -0.1182, -0.1147,  0.0921, -0.0033,\n",
       "          0.2048,  0.1877, -0.1732, -0.0861,  0.2154,  0.0813, -0.0482, -0.1471,\n",
       "          0.0729,  0.1672, -0.2414,  0.1081, -0.0761, -0.1497, -0.0425,  0.0763,\n",
       "          0.0946, -0.1367,  0.0379, -0.0749, -0.2108,  0.0960,  0.0245,  0.2244,\n",
       "          0.1602, -0.2316, -0.0304, -0.1809, -0.0896,  0.2048,  0.2831,  0.2165,\n",
       "         -0.2307, -0.1157, -0.0104,  0.0336,  0.1026,  0.2040,  0.0713, -0.2639,\n",
       "          0.0778,  0.0500,  0.1923,  0.1277,  0.1815, -0.2308,  0.1551, -0.2286,\n",
       "         -0.2252, -0.0512,  0.2110,  0.0355, -0.1942,  0.1886, -0.1394,  0.0509,\n",
       "          0.1202, -0.1873, -0.0009, -0.0283,  0.2121,  0.0804,  0.1885,  0.2071,\n",
       "         -0.1146,  0.1334,  0.0909,  0.1458,  0.0371, -0.1957,  0.2245, -0.2641,\n",
       "         -0.0151, -0.0276, -0.0836, -0.1191, -0.0810,  0.1114, -0.1471, -0.0285,\n",
       "         -0.0704,  0.1866,  0.2525,  0.0854,  0.1541, -0.1246, -0.2074,  0.1898,\n",
       "          0.2544,  0.1057,  0.0266, -0.2380,  0.2070,  0.0359,  0.0576,  0.1031,\n",
       "          0.1448, -0.2236, -0.0662,  0.1450, -0.1213, -0.1980, -0.0199,  0.1476,\n",
       "          0.1931, -0.1353,  0.1945, -0.0050,  0.0417, -0.0790, -0.1385,  0.1613,\n",
       "          0.1442,  0.0873, -0.1752,  0.1793,  0.2524, -0.2422,  0.1832,  0.2125,\n",
       "          0.0878, -0.2427, -0.1157, -0.1772,  0.1735,  0.0131,  0.0559, -0.0500,\n",
       "         -0.1939, -0.1549,  0.1681,  0.2576,  0.0758,  0.0899,  0.0342, -0.0172,\n",
       "         -0.1564, -0.2578,  0.1180, -0.2498, -0.0539, -0.1182, -0.0336, -0.1141,\n",
       "          0.0446, -0.1486,  0.1746,  0.0762, -0.1405,  0.0448, -0.2609,  0.0096,\n",
       "         -0.0782,  0.2262,  0.2258, -0.0248,  0.0938, -0.2441, -0.0882,  0.0792,\n",
       "         -0.1055, -0.1155,  0.0341,  0.1175,  0.0215, -0.0857, -0.1695, -0.1577,\n",
       "         -0.1969,  0.1435, -0.2649,  0.1795, -0.1148,  0.2169,  0.0949, -0.2064,\n",
       "          0.0621, -0.1601,  0.2444, -0.0065,  0.1467,  0.0220,  0.0661,  0.1205,\n",
       "          0.0683, -0.1975,  0.0651,  0.2035,  0.1648, -0.2009, -0.2092, -0.2035,\n",
       "          0.0442,  0.2364, -0.2322, -0.1091, -0.1303, -0.0804, -0.1521,  0.0437,\n",
       "         -0.0293, -0.0676,  0.2063, -0.2415, -0.0775,  0.0658,  0.1934,  0.1241,\n",
       "          0.2322,  0.0168, -0.0104,  0.0872,  0.0879, -0.1017, -0.0718, -0.1307,\n",
       "          0.2594, -0.2004,  0.1750,  0.0769,  0.2043,  0.0724, -0.2261,  0.0856,\n",
       "         -0.1057, -0.2325,  0.1403,  0.0057,  0.1419,  0.0259,  0.1827, -0.0589,\n",
       "          0.0746, -0.0092, -0.0078, -0.1230,  0.1779,  0.1347, -0.0604,  0.1400,\n",
       "         -0.0195,  0.1195, -0.2178,  0.2222,  0.0282, -0.0979,  0.0141, -0.0208,\n",
       "          0.0783,  0.0834,  0.1010,  0.1194,  0.0306,  0.1227, -0.0238, -0.0877,\n",
       "         -0.1093,  0.0120,  0.2259,  0.1914, -0.1404,  0.1296, -0.1403,  0.1636,\n",
       "         -0.2438,  0.1688,  0.2359, -0.0039,  0.1959, -0.2448,  0.1091, -0.0898,\n",
       "         -0.0822, -0.1436, -0.0462,  0.1166, -0.1012,  0.2074,  0.0394,  0.0921],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 4.1278e-02, -6.7849e-03, -4.5960e-02,  ...,  6.2805e-05,\n",
       "          -2.6595e-02, -1.8976e-02],\n",
       "         [ 1.7838e-03, -2.4043e-02,  7.7809e-04,  ..., -7.5658e-02,\n",
       "           5.3687e-02, -8.9672e-03],\n",
       "         [ 1.0132e-02, -3.3095e-02, -2.9559e-02,  ..., -4.8992e-02,\n",
       "          -7.1659e-03,  2.1242e-02],\n",
       "         ...,\n",
       "         [-1.7378e-02, -8.9730e-03, -1.6638e-02,  ..., -2.5265e-02,\n",
       "           2.3593e-02,  1.8103e-02],\n",
       "         [ 3.5799e-02,  8.1830e-02,  2.2687e-03,  ...,  6.2357e-02,\n",
       "           1.8346e-02,  4.6302e-02],\n",
       "         [ 8.7989e-02,  4.6054e-02,  4.5712e-02,  ...,  2.1118e-02,\n",
       "          -5.3984e-02,  1.1680e-01]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0384,  0.0524,  0.0343, -0.0506,  0.0003, -0.0071, -0.0294, -0.0185,\n",
       "          0.0118,  0.0057, -0.0228, -0.0090,  0.0057,  0.0404, -0.0217,  0.0036,\n",
       "         -0.0181,  0.0074, -0.0317, -0.0342,  0.0071, -0.0288, -0.0408,  0.0442,\n",
       "          0.0342, -0.0332, -0.0362,  0.0176, -0.0375,  0.0318,  0.0404,  0.0458,\n",
       "          0.0078,  0.0040,  0.0200, -0.0366,  0.0267, -0.0022,  0.0354, -0.0474,\n",
       "         -0.0166, -0.0006, -0.0007,  0.0400, -0.0372, -0.0399, -0.0033, -0.0008,\n",
       "          0.0100, -0.0250,  0.0383,  0.0199,  0.0128,  0.0305,  0.0343, -0.0221,\n",
       "          0.0687,  0.0022,  0.0105,  0.0151,  0.0002, -0.0363,  0.0017,  0.0028,\n",
       "          0.0230,  0.0226, -0.0056, -0.0060, -0.0110, -0.0195, -0.0237,  0.0310,\n",
       "          0.0229,  0.0102, -0.0455,  0.0325,  0.0163,  0.0451,  0.0147, -0.0333,\n",
       "         -0.0341, -0.0206, -0.0069,  0.0169,  0.0133, -0.0024, -0.0069, -0.0051,\n",
       "         -0.0018, -0.0373,  0.0120,  0.0156,  0.0120, -0.0171, -0.0177,  0.0141,\n",
       "         -0.0035, -0.0330, -0.0130, -0.0467, -0.0018,  0.0134, -0.0195, -0.0312,\n",
       "          0.0419,  0.0307,  0.0108, -0.0144,  0.0031,  0.0107, -0.0223,  0.0073,\n",
       "          0.0363, -0.0120, -0.0274, -0.0069,  0.0219,  0.0114, -0.0360,  0.0247,\n",
       "         -0.0016,  0.0057, -0.0445, -0.0320, -0.0266,  0.0085,  0.0428,  0.0379,\n",
       "         -0.0180,  0.0529,  0.0405, -0.0164,  0.0095, -0.0098,  0.0112,  0.0060,\n",
       "          0.0351, -0.0354,  0.0060, -0.0238, -0.0246,  0.0400,  0.0024,  0.0903,\n",
       "          0.0025,  0.0171, -0.0017, -0.0331, -0.0256,  0.0092, -0.0064, -0.0089,\n",
       "         -0.0344,  0.0494, -0.0250, -0.0384, -0.0258, -0.0120, -0.0347,  0.0508,\n",
       "         -0.0008, -0.0334, -0.0415,  0.0257,  0.0609,  0.0077, -0.0350, -0.0118,\n",
       "         -0.0241,  0.0029,  0.0233, -0.0546, -0.0086, -0.0486, -0.0328,  0.0228,\n",
       "         -0.0439, -0.0425, -0.0320,  0.0123, -0.0224,  0.0212, -0.0295,  0.0227,\n",
       "          0.0373,  0.0509, -0.0486,  0.0102,  0.0256,  0.0283, -0.0049,  0.0052,\n",
       "         -0.0157,  0.0483,  0.0189,  0.0068, -0.0114,  0.0213, -0.0225,  0.0361,\n",
       "         -0.0154, -0.0002,  0.0180, -0.0240,  0.0241,  0.0006,  0.0318, -0.0221,\n",
       "         -0.0064, -0.0498,  0.0070,  0.0372, -0.0281, -0.0068, -0.0252, -0.0090,\n",
       "          0.0113,  0.0234, -0.0158,  0.0324, -0.0249, -0.0172, -0.0205, -0.0013,\n",
       "          0.0095, -0.0057, -0.0430, -0.0238, -0.0108, -0.0365, -0.0168,  0.0380,\n",
       "         -0.0142,  0.0281, -0.0181,  0.0036,  0.0233,  0.0110,  0.0047,  0.0148,\n",
       "         -0.0202,  0.0722, -0.0015, -0.0202,  0.0105, -0.0281, -0.0308, -0.0169,\n",
       "          0.0263,  0.0244, -0.0248,  0.0092,  0.0101,  0.0149,  0.0076, -0.0055],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 5.4633e-02,  8.1261e-03, -3.0832e-02,  ...,  5.4763e-02,\n",
       "          -3.9070e-02, -4.7520e-02],\n",
       "         [ 7.2382e-02, -4.7307e-02, -2.3797e-02,  ...,  3.9055e-03,\n",
       "          -1.5235e-02, -1.7146e-02],\n",
       "         [-1.7924e-02,  4.6757e-04, -4.0349e-02,  ...,  5.7521e-02,\n",
       "           9.0531e-02, -2.5051e-03],\n",
       "         ...,\n",
       "         [ 1.4053e-02, -4.3309e-02,  3.2839e-05,  ..., -2.4532e-02,\n",
       "          -2.0915e-02,  6.3625e-02],\n",
       "         [-5.1204e-03,  4.1185e-02,  1.7763e-02,  ...,  2.1898e-03,\n",
       "          -7.2193e-02,  1.9890e-02],\n",
       "         [-5.4119e-02,  2.7216e-02, -4.5889e-02,  ...,  1.9468e-03,\n",
       "           1.6108e-02,  3.3428e-02]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0218,  0.0122, -0.0090,  0.0192,  0.0073, -0.0146, -0.0172, -0.0161,\n",
       "          0.0034, -0.0175, -0.0405,  0.0299, -0.0603, -0.0260,  0.0148, -0.0506,\n",
       "          0.0101,  0.0233,  0.0611,  0.0053,  0.0120,  0.0282, -0.0101, -0.0045,\n",
       "          0.0508,  0.0130, -0.0003, -0.0131, -0.0401,  0.0463, -0.0210,  0.0293,\n",
       "         -0.0256,  0.0346,  0.0168, -0.0242, -0.0049, -0.0485,  0.0420,  0.0150,\n",
       "          0.0399, -0.0609, -0.0609,  0.0046,  0.0419, -0.0323, -0.0594,  0.0583,\n",
       "         -0.0254, -0.0278,  0.0016, -0.0256, -0.0305,  0.0366, -0.0453,  0.0189,\n",
       "         -0.0292,  0.0077,  0.0030, -0.0285], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0554, -0.0423, -0.1156,  0.0730, -0.0532,  0.0089, -0.0901, -0.1038,\n",
       "          -0.1392,  0.0981,  0.0516,  0.1187, -0.0476, -0.1402,  0.0835,  0.0997,\n",
       "          -0.1758,  0.1161,  0.1730,  0.0904, -0.0909,  0.0716, -0.0481, -0.0731,\n",
       "          -0.1037, -0.0566, -0.0789,  0.0637,  0.0840, -0.0526,  0.0498,  0.0710,\n",
       "          -0.0173,  0.0521, -0.0801,  0.1083, -0.1271,  0.0110,  0.1764, -0.1537,\n",
       "          -0.1162, -0.0945, -0.1474,  0.1394,  0.0944,  0.0190,  0.0964, -0.1173,\n",
       "          -0.1240,  0.1212,  0.1095, -0.0452,  0.0376, -0.1081, -0.0215, -0.1443,\n",
       "           0.0689, -0.1461,  0.1446, -0.0005]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0053], requires_grad=True)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP\n",
    "\n",
    "list(filter(lambda p: p.requires_grad,model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51b00e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001430E7469E0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddbf1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95190c79",
   "metadata": {},
   "source": [
    "# Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67e6a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import OrderedDict\n",
    "from typing import Dict, List, OrderedDict, Tuple, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2b2c14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_of_model_2='../../WW_BW_BM.pth'\n",
    "model_2 = DeepNet()\n",
    "model_2.load_state_dict(torch.load(name_of_model_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63708615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('layer1.weight', Parameter containing:\n",
       "tensor([[-0.1236,  0.0936, -0.0858,  ..., -0.2022,  0.2071,  0.2143],\n",
       "        [-0.0173, -0.0282,  0.0289,  ..., -0.0473,  0.0984, -0.1676],\n",
       "        [ 0.2257, -0.0810, -0.0682,  ..., -0.0027, -0.1340, -0.0711],\n",
       "        ...,\n",
       "        [-0.0892, -0.2357,  0.0203,  ...,  0.2679, -0.0388, -0.2460],\n",
       "        [ 0.2170,  0.2365,  0.2169,  ..., -0.1546,  0.0313,  0.2786],\n",
       "        [-0.0207, -0.1458, -0.1536,  ...,  0.0709, -0.0735, -0.1318]],\n",
       "       requires_grad=True)), ('layer1.bias', Parameter containing:\n",
       "tensor([-0.0322,  0.0426, -0.0603,  0.0798,  0.1138,  0.1408,  0.0317,  0.2289,\n",
       "         0.1325, -0.1165, -0.1265,  0.2285, -0.0939,  0.1958,  0.2569,  0.1675,\n",
       "        -0.2616,  0.1116,  0.1401, -0.0685, -0.1891, -0.1634, -0.1235,  0.0884,\n",
       "        -0.2367,  0.2376, -0.0763, -0.0003,  0.1408, -0.0229,  0.1229, -0.1721,\n",
       "         0.0450, -0.2093,  0.0882, -0.2293, -0.2588,  0.1078, -0.0765,  0.0842,\n",
       "        -0.0346, -0.0134, -0.2554,  0.0831, -0.1555,  0.0582,  0.1946, -0.2204,\n",
       "         0.2441,  0.1677, -0.0461, -0.1893,  0.2440,  0.1668, -0.0349, -0.1506,\n",
       "        -0.1688,  0.2263,  0.0252, -0.1341,  0.1226,  0.0148, -0.2670, -0.2387,\n",
       "        -0.1646,  0.0938,  0.2166, -0.1961,  0.2032,  0.2083, -0.0538, -0.1734,\n",
       "        -0.0966,  0.1707,  0.2654, -0.1752,  0.1697,  0.0705,  0.0117, -0.1214,\n",
       "        -0.0225,  0.2655,  0.0326, -0.2243,  0.0494,  0.0262,  0.1043, -0.2016,\n",
       "        -0.1868,  0.2294,  0.1924, -0.0286,  0.1204,  0.0552,  0.1580,  0.2101,\n",
       "         0.0064,  0.1496,  0.0149, -0.2476,  0.2136, -0.2267, -0.1353, -0.2492,\n",
       "        -0.0971,  0.0536,  0.0151, -0.1011, -0.1989,  0.1396, -0.0581, -0.0835,\n",
       "        -0.0924,  0.0628,  0.0724, -0.1092,  0.1702, -0.1673, -0.1401, -0.0940,\n",
       "         0.1129,  0.2617, -0.1878,  0.0058, -0.0888,  0.1451, -0.1095, -0.1658,\n",
       "        -0.0124,  0.0627, -0.1805, -0.2139,  0.1665, -0.2399,  0.1778,  0.0173,\n",
       "         0.2214, -0.2617,  0.1081, -0.1959,  0.0678, -0.0350,  0.1809, -0.1450,\n",
       "         0.2169, -0.1114,  0.1557,  0.0868, -0.0219, -0.0083, -0.1890, -0.0412,\n",
       "        -0.2220,  0.1934,  0.2318, -0.0247, -0.1509, -0.0488, -0.0465,  0.0737,\n",
       "        -0.2061, -0.2349,  0.2161,  0.2344,  0.2316, -0.1617,  0.0875,  0.0097,\n",
       "        -0.0420,  0.0644, -0.1184,  0.0654, -0.1385,  0.0454,  0.2524,  0.1664,\n",
       "         0.1929, -0.1204,  0.2308, -0.1499,  0.0881,  0.0181,  0.2450, -0.1845,\n",
       "         0.1118, -0.0929,  0.0865,  0.2135, -0.0189, -0.0089,  0.1641,  0.0661,\n",
       "         0.1976, -0.1441,  0.0790,  0.2366,  0.0260, -0.0965, -0.1012, -0.1548,\n",
       "         0.1602, -0.0468, -0.2533,  0.1919,  0.1327,  0.1548, -0.1613,  0.1136,\n",
       "         0.1652, -0.1754,  0.1503, -0.0373, -0.2608,  0.1419, -0.2441,  0.2094,\n",
       "        -0.2494,  0.0223, -0.2322,  0.0267, -0.1226, -0.0819,  0.0992, -0.0039,\n",
       "         0.1870, -0.0248,  0.2247,  0.0420, -0.1182, -0.1147,  0.0921, -0.0033,\n",
       "         0.2048,  0.1877, -0.1732, -0.0861,  0.2154,  0.0813, -0.0482, -0.1471,\n",
       "         0.0729,  0.1672, -0.2414,  0.1081, -0.0761, -0.1497, -0.0425,  0.0763,\n",
       "         0.0946, -0.1367,  0.0379, -0.0749, -0.2108,  0.0960,  0.0245,  0.2244,\n",
       "         0.1602, -0.2316, -0.0304, -0.1809, -0.0896,  0.2048,  0.2831,  0.2165,\n",
       "        -0.2307, -0.1157, -0.0104,  0.0336,  0.1026,  0.2040,  0.0713, -0.2639,\n",
       "         0.0778,  0.0500,  0.1923,  0.1277,  0.1815, -0.2308,  0.1551, -0.2286,\n",
       "        -0.2252, -0.0512,  0.2110,  0.0355, -0.1942,  0.1886, -0.1394,  0.0509,\n",
       "         0.1202, -0.1873, -0.0009, -0.0283,  0.2121,  0.0804,  0.1885,  0.2071,\n",
       "        -0.1146,  0.1334,  0.0909,  0.1458,  0.0371, -0.1957,  0.2245, -0.2641,\n",
       "        -0.0151, -0.0276, -0.0836, -0.1191, -0.0810,  0.1114, -0.1471, -0.0285,\n",
       "        -0.0704,  0.1866,  0.2525,  0.0854,  0.1541, -0.1246, -0.2074,  0.1898,\n",
       "         0.2544,  0.1057,  0.0266, -0.2380,  0.2070,  0.0359,  0.0576,  0.1031,\n",
       "         0.1448, -0.2236, -0.0662,  0.1450, -0.1213, -0.1980, -0.0199,  0.1476,\n",
       "         0.1931, -0.1353,  0.1945, -0.0050,  0.0417, -0.0790, -0.1385,  0.1613,\n",
       "         0.1442,  0.0873, -0.1752,  0.1793,  0.2524, -0.2422,  0.1832,  0.2125,\n",
       "         0.0878, -0.2427, -0.1157, -0.1772,  0.1735,  0.0131,  0.0559, -0.0500,\n",
       "        -0.1939, -0.1549,  0.1681,  0.2576,  0.0758,  0.0899,  0.0342, -0.0172,\n",
       "        -0.1564, -0.2578,  0.1180, -0.2498, -0.0539, -0.1182, -0.0336, -0.1141,\n",
       "         0.0446, -0.1486,  0.1746,  0.0762, -0.1405,  0.0448, -0.2609,  0.0096,\n",
       "        -0.0782,  0.2262,  0.2258, -0.0248,  0.0938, -0.2441, -0.0882,  0.0792,\n",
       "        -0.1055, -0.1155,  0.0341,  0.1175,  0.0215, -0.0857, -0.1695, -0.1577,\n",
       "        -0.1969,  0.1435, -0.2649,  0.1795, -0.1148,  0.2169,  0.0949, -0.2064,\n",
       "         0.0621, -0.1601,  0.2444, -0.0065,  0.1467,  0.0220,  0.0661,  0.1205,\n",
       "         0.0683, -0.1975,  0.0651,  0.2035,  0.1648, -0.2009, -0.2092, -0.2035,\n",
       "         0.0442,  0.2364, -0.2322, -0.1091, -0.1303, -0.0804, -0.1521,  0.0437,\n",
       "        -0.0293, -0.0676,  0.2063, -0.2415, -0.0775,  0.0658,  0.1934,  0.1241,\n",
       "         0.2322,  0.0168, -0.0104,  0.0872,  0.0879, -0.1017, -0.0718, -0.1307,\n",
       "         0.2594, -0.2004,  0.1750,  0.0769,  0.2043,  0.0724, -0.2261,  0.0856,\n",
       "        -0.1057, -0.2325,  0.1403,  0.0057,  0.1419,  0.0259,  0.1827, -0.0589,\n",
       "         0.0746, -0.0092, -0.0078, -0.1230,  0.1779,  0.1347, -0.0604,  0.1400,\n",
       "        -0.0195,  0.1195, -0.2178,  0.2222,  0.0282, -0.0979,  0.0141, -0.0208,\n",
       "         0.0783,  0.0834,  0.1010,  0.1194,  0.0306,  0.1227, -0.0238, -0.0877,\n",
       "        -0.1093,  0.0120,  0.2259,  0.1914, -0.1404,  0.1296, -0.1403,  0.1636,\n",
       "        -0.2438,  0.1688,  0.2359, -0.0039,  0.1959, -0.2448,  0.1091, -0.0898,\n",
       "        -0.0822, -0.1436, -0.0462,  0.1166, -0.1012,  0.2074,  0.0394,  0.0921],\n",
       "       requires_grad=True)), ('layer2.weight', Parameter containing:\n",
       "tensor([[ 4.1278e-02, -6.7849e-03, -4.5960e-02,  ...,  6.2805e-05,\n",
       "         -2.6595e-02, -1.8976e-02],\n",
       "        [ 1.7838e-03, -2.4043e-02,  7.7809e-04,  ..., -7.5658e-02,\n",
       "          5.3687e-02, -8.9672e-03],\n",
       "        [ 1.0132e-02, -3.3095e-02, -2.9559e-02,  ..., -4.8992e-02,\n",
       "         -7.1659e-03,  2.1242e-02],\n",
       "        ...,\n",
       "        [-1.7378e-02, -8.9730e-03, -1.6638e-02,  ..., -2.5265e-02,\n",
       "          2.3593e-02,  1.8103e-02],\n",
       "        [ 3.5799e-02,  8.1830e-02,  2.2687e-03,  ...,  6.2357e-02,\n",
       "          1.8346e-02,  4.6302e-02],\n",
       "        [ 8.7989e-02,  4.6054e-02,  4.5712e-02,  ...,  2.1118e-02,\n",
       "         -5.3984e-02,  1.1680e-01]], requires_grad=True)), ('layer2.bias', Parameter containing:\n",
       "tensor([ 0.0384,  0.0524,  0.0343, -0.0506,  0.0003, -0.0071, -0.0294, -0.0185,\n",
       "         0.0118,  0.0057, -0.0228, -0.0090,  0.0057,  0.0404, -0.0217,  0.0036,\n",
       "        -0.0181,  0.0074, -0.0317, -0.0342,  0.0071, -0.0288, -0.0408,  0.0442,\n",
       "         0.0342, -0.0332, -0.0362,  0.0176, -0.0375,  0.0318,  0.0404,  0.0458,\n",
       "         0.0078,  0.0040,  0.0200, -0.0366,  0.0267, -0.0022,  0.0354, -0.0474,\n",
       "        -0.0166, -0.0006, -0.0007,  0.0400, -0.0372, -0.0399, -0.0033, -0.0008,\n",
       "         0.0100, -0.0250,  0.0383,  0.0199,  0.0128,  0.0305,  0.0343, -0.0221,\n",
       "         0.0687,  0.0022,  0.0105,  0.0151,  0.0002, -0.0363,  0.0017,  0.0028,\n",
       "         0.0230,  0.0226, -0.0056, -0.0060, -0.0110, -0.0195, -0.0237,  0.0310,\n",
       "         0.0229,  0.0102, -0.0455,  0.0325,  0.0163,  0.0451,  0.0147, -0.0333,\n",
       "        -0.0341, -0.0206, -0.0069,  0.0169,  0.0133, -0.0024, -0.0069, -0.0051,\n",
       "        -0.0018, -0.0373,  0.0120,  0.0156,  0.0120, -0.0171, -0.0177,  0.0141,\n",
       "        -0.0035, -0.0330, -0.0130, -0.0467, -0.0018,  0.0134, -0.0195, -0.0312,\n",
       "         0.0419,  0.0307,  0.0108, -0.0144,  0.0031,  0.0107, -0.0223,  0.0073,\n",
       "         0.0363, -0.0120, -0.0274, -0.0069,  0.0219,  0.0114, -0.0360,  0.0247,\n",
       "        -0.0016,  0.0057, -0.0445, -0.0320, -0.0266,  0.0085,  0.0428,  0.0379,\n",
       "        -0.0180,  0.0529,  0.0405, -0.0164,  0.0095, -0.0098,  0.0112,  0.0060,\n",
       "         0.0351, -0.0354,  0.0060, -0.0238, -0.0246,  0.0400,  0.0024,  0.0903,\n",
       "         0.0025,  0.0171, -0.0017, -0.0331, -0.0256,  0.0092, -0.0064, -0.0089,\n",
       "        -0.0344,  0.0494, -0.0250, -0.0384, -0.0258, -0.0120, -0.0347,  0.0508,\n",
       "        -0.0008, -0.0334, -0.0415,  0.0257,  0.0609,  0.0077, -0.0350, -0.0118,\n",
       "        -0.0241,  0.0029,  0.0233, -0.0546, -0.0086, -0.0486, -0.0328,  0.0228,\n",
       "        -0.0439, -0.0425, -0.0320,  0.0123, -0.0224,  0.0212, -0.0295,  0.0227,\n",
       "         0.0373,  0.0509, -0.0486,  0.0102,  0.0256,  0.0283, -0.0049,  0.0052,\n",
       "        -0.0157,  0.0483,  0.0189,  0.0068, -0.0114,  0.0213, -0.0225,  0.0361,\n",
       "        -0.0154, -0.0002,  0.0180, -0.0240,  0.0241,  0.0006,  0.0318, -0.0221,\n",
       "        -0.0064, -0.0498,  0.0070,  0.0372, -0.0281, -0.0068, -0.0252, -0.0090,\n",
       "         0.0113,  0.0234, -0.0158,  0.0324, -0.0249, -0.0172, -0.0205, -0.0013,\n",
       "         0.0095, -0.0057, -0.0430, -0.0238, -0.0108, -0.0365, -0.0168,  0.0380,\n",
       "        -0.0142,  0.0281, -0.0181,  0.0036,  0.0233,  0.0110,  0.0047,  0.0148,\n",
       "        -0.0202,  0.0722, -0.0015, -0.0202,  0.0105, -0.0281, -0.0308, -0.0169,\n",
       "         0.0263,  0.0244, -0.0248,  0.0092,  0.0101,  0.0149,  0.0076, -0.0055],\n",
       "       requires_grad=True)), ('layer3.weight', Parameter containing:\n",
       "tensor([[ 5.4633e-02,  8.1261e-03, -3.0832e-02,  ...,  5.4763e-02,\n",
       "         -3.9070e-02, -4.7520e-02],\n",
       "        [ 7.2382e-02, -4.7307e-02, -2.3797e-02,  ...,  3.9055e-03,\n",
       "         -1.5235e-02, -1.7146e-02],\n",
       "        [-1.7924e-02,  4.6757e-04, -4.0349e-02,  ...,  5.7521e-02,\n",
       "          9.0531e-02, -2.5051e-03],\n",
       "        ...,\n",
       "        [ 1.4053e-02, -4.3309e-02,  3.2839e-05,  ..., -2.4532e-02,\n",
       "         -2.0915e-02,  6.3625e-02],\n",
       "        [-5.1204e-03,  4.1185e-02,  1.7763e-02,  ...,  2.1898e-03,\n",
       "         -7.2193e-02,  1.9890e-02],\n",
       "        [-5.4119e-02,  2.7216e-02, -4.5889e-02,  ...,  1.9468e-03,\n",
       "          1.6108e-02,  3.3428e-02]], requires_grad=True)), ('layer3.bias', Parameter containing:\n",
       "tensor([ 0.0218,  0.0122, -0.0090,  0.0192,  0.0073, -0.0146, -0.0172, -0.0161,\n",
       "         0.0034, -0.0175, -0.0405,  0.0299, -0.0603, -0.0260,  0.0148, -0.0506,\n",
       "         0.0101,  0.0233,  0.0611,  0.0053,  0.0120,  0.0282, -0.0101, -0.0045,\n",
       "         0.0508,  0.0130, -0.0003, -0.0131, -0.0401,  0.0463, -0.0210,  0.0293,\n",
       "        -0.0256,  0.0346,  0.0168, -0.0242, -0.0049, -0.0485,  0.0420,  0.0150,\n",
       "         0.0399, -0.0609, -0.0609,  0.0046,  0.0419, -0.0323, -0.0594,  0.0583,\n",
       "        -0.0254, -0.0278,  0.0016, -0.0256, -0.0305,  0.0366, -0.0453,  0.0189,\n",
       "        -0.0292,  0.0077,  0.0030, -0.0285], requires_grad=True)), ('output.weight', Parameter containing:\n",
       "tensor([[ 0.0554, -0.0423, -0.1156,  0.0730, -0.0532,  0.0089, -0.0901, -0.1038,\n",
       "         -0.1392,  0.0981,  0.0516,  0.1187, -0.0476, -0.1402,  0.0835,  0.0997,\n",
       "         -0.1758,  0.1161,  0.1730,  0.0904, -0.0909,  0.0716, -0.0481, -0.0731,\n",
       "         -0.1037, -0.0566, -0.0789,  0.0637,  0.0840, -0.0526,  0.0498,  0.0710,\n",
       "         -0.0173,  0.0521, -0.0801,  0.1083, -0.1271,  0.0110,  0.1764, -0.1537,\n",
       "         -0.1162, -0.0945, -0.1474,  0.1394,  0.0944,  0.0190,  0.0964, -0.1173,\n",
       "         -0.1240,  0.1212,  0.1095, -0.0452,  0.0376, -0.1081, -0.0215, -0.1443,\n",
       "          0.0689, -0.1461,  0.1446, -0.0005]], requires_grad=True)), ('output.bias', Parameter containing:\n",
       "tensor([-0.0053], requires_grad=True))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict(keep_vars=True).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce18065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_parameters(\n",
    "    src: Union[OrderedDict[str, torch.Tensor], torch.nn.Module]\n",
    ") -> OrderedDict[str, torch.Tensor]:\n",
    "    if isinstance(src, OrderedDict):\n",
    "        return OrderedDict(\n",
    "            {\n",
    "                name: param.clone().detach().requires_grad_(param.requires_grad)\n",
    "                for name, param in src.items()\n",
    "            }\n",
    "        )\n",
    "    if isinstance(src, torch.nn.Module):\n",
    "        return OrderedDict(\n",
    "            {\n",
    "                name: param.clone().detach().requires_grad_(param.requires_grad)\n",
    "                for name, param in src.state_dict(keep_vars=True).items()\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4b4326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = clone_parameters(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9c1b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = OrderedDict(\n",
    "            {\n",
    "                k: p1 - p0 \n",
    "                for (k, p1), p0 in zip(\n",
    "                    model.state_dict(keep_vars=True).items(),\n",
    "                    model_2.values(),\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d88568d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[-0.1099,  0.2193,  0.1262,  ..., -0.3510,  0.3618, -0.0067],\n",
       "                      [-0.2329,  0.2144, -0.2351,  ...,  0.1723,  0.0014, -0.4251],\n",
       "                      [ 0.2713,  0.1202,  0.1524,  ..., -0.0205,  0.1232, -0.0353],\n",
       "                      ...,\n",
       "                      [-0.1317, -0.0478, -0.1220,  ...,  0.4639, -0.0288, -0.4378],\n",
       "                      [ 0.3609,  0.4064,  0.3201,  ..., -0.4533, -0.2350,  0.3382],\n",
       "                      [ 0.1076, -0.2068, -0.1004,  ...,  0.2162, -0.2199,  0.1137]],\n",
       "                     grad_fn=<SubBackward0>)),\n",
       "             ('layer1.bias',\n",
       "              tensor([-2.6606e-01,  8.0094e-02,  1.5489e-01, -3.7381e-02,  1.5904e-01,\n",
       "                       3.4336e-01,  7.7128e-02,  1.3422e-01,  5.8573e-02, -1.7140e-01,\n",
       "                      -1.5894e-01,  4.9366e-02, -1.6139e-01,  4.2377e-01,  2.5962e-01,\n",
       "                       3.8594e-01,  2.3428e-04,  1.1074e-01, -3.7420e-02,  1.0180e-01,\n",
       "                      -8.4533e-02, -2.9983e-01, -3.0974e-01, -6.7809e-02, -1.0335e-01,\n",
       "                       1.4939e-01,  1.5206e-03, -6.8679e-02,  2.3903e-01, -2.5304e-01,\n",
       "                       1.1763e-01, -1.3861e-01,  3.7113e-02, -1.3088e-01, -3.4809e-03,\n",
       "                      -3.4593e-02, -3.0363e-01, -1.0396e-01, -9.4287e-02, -2.5993e-02,\n",
       "                       4.6202e-03,  2.3940e-01, -4.7408e-03, -6.9745e-02, -2.9795e-01,\n",
       "                       6.8826e-02,  5.2584e-02, -2.3755e-01,  2.9616e-01,  2.3976e-01,\n",
       "                       9.5537e-02, -2.5347e-01,  4.7429e-01,  1.8688e-01, -1.0805e-01,\n",
       "                      -5.8474e-02,  1.7349e-02,  1.9001e-01, -2.3146e-01,  1.1098e-01,\n",
       "                      -7.5591e-02, -3.5371e-02, -1.0449e-01, -4.8144e-01, -5.8054e-03,\n",
       "                       1.4233e-01,  2.4224e-01, -2.3404e-02, -2.8044e-02,  1.5257e-01,\n",
       "                       1.2203e-01, -2.5592e-01,  1.6769e-01,  1.1430e-01,  2.0597e-01,\n",
       "                      -1.0305e-01,  3.5381e-01, -7.7584e-02,  1.2018e-01,  8.4616e-02,\n",
       "                      -2.7974e-01,  1.0611e-02, -1.9872e-01, -2.7289e-01,  3.1574e-01,\n",
       "                      -6.1905e-02,  2.6808e-01, -3.2311e-01, -2.9171e-01,  2.8067e-01,\n",
       "                       2.9106e-02,  2.1172e-02, -9.7850e-02,  2.5136e-01,  1.8668e-01,\n",
       "                       1.0008e-01, -1.3061e-01, -6.7636e-02,  1.1773e-01, -4.4840e-01,\n",
       "                       1.9939e-01, -2.6674e-01, -4.0311e-01, -1.8326e-01, -2.4311e-01,\n",
       "                       2.1457e-01,  1.0082e-01, -3.4216e-01, -3.1101e-01,  5.1516e-02,\n",
       "                       1.4642e-01, -6.8499e-03,  3.5643e-02,  8.4330e-02, -2.8571e-02,\n",
       "                      -6.6823e-02, -3.5623e-02, -2.6907e-01, -2.9418e-01, -2.8068e-01,\n",
       "                      -1.0774e-01,  4.5768e-01, -2.6910e-01,  1.6334e-01, -2.5100e-01,\n",
       "                       3.9463e-01, -2.1008e-02, -2.4147e-02,  1.8952e-01,  2.5483e-01,\n",
       "                      -4.3096e-01, -2.2485e-02,  3.2489e-01,  2.4610e-02,  3.6579e-01,\n",
       "                      -1.6296e-02,  3.4654e-01, -2.7632e-02,  2.5355e-01, -2.7679e-01,\n",
       "                      -5.2363e-02, -3.3987e-02,  1.6336e-01,  6.9763e-02,  3.4235e-01,\n",
       "                       7.6543e-02,  4.6218e-02,  8.7230e-03,  2.4292e-01,  2.0350e-01,\n",
       "                      -3.2286e-01, -1.2904e-01, -4.2265e-01, -5.6781e-03,  4.3631e-01,\n",
       "                       2.8345e-02, -3.5071e-01,  8.1894e-02, -2.8229e-01, -1.0508e-01,\n",
       "                      -5.2513e-02, -2.8439e-01,  3.3375e-01,  4.8676e-01,  3.8084e-01,\n",
       "                      -4.2336e-01,  1.6728e-01,  1.6446e-01,  3.8607e-02,  1.5764e-01,\n",
       "                      -2.8742e-01, -3.1356e-02, -1.7861e-01,  2.5618e-01,  4.5337e-01,\n",
       "                       2.9692e-01,  3.1406e-01, -2.1012e-01,  4.0147e-01,  2.0689e-02,\n",
       "                       1.8705e-01,  1.2881e-01,  2.2558e-01, -2.1599e-01,  8.5608e-02,\n",
       "                       9.3124e-02, -2.4782e-02,  2.0264e-02,  8.7754e-02, -6.2912e-02,\n",
       "                       1.0349e-01,  1.3590e-01,  2.4523e-01,  6.8415e-02,  1.7763e-01,\n",
       "                       1.0884e-01,  2.2174e-01, -1.8054e-01, -2.8243e-02, -2.7220e-01,\n",
       "                       2.7310e-02, -3.0860e-01, -4.7940e-01,  1.9750e-01, -4.7858e-02,\n",
       "                       7.7075e-02, -1.2621e-01, -5.8184e-02, -4.0416e-02, -3.2453e-01,\n",
       "                       2.9343e-01, -2.0118e-01, -3.0293e-02, -5.9782e-02, -4.0666e-01,\n",
       "                       8.6981e-02, -4.5431e-01,  1.3140e-01, -4.4292e-01,  6.6268e-02,\n",
       "                      -2.8243e-01, -2.8341e-01, -1.4416e-01,  2.0011e-01,  7.4965e-02,\n",
       "                      -2.1024e-01,  3.8437e-01,  8.7096e-02, -2.1170e-01, -2.9916e-01,\n",
       "                       2.2905e-01,  1.9397e-01, -1.8038e-02, -5.3811e-03, -3.1596e-02,\n",
       "                      -1.4603e-01,  1.4552e-01,  2.7181e-01, -1.2732e-01, -3.6471e-01,\n",
       "                       3.2992e-03,  1.8411e-01, -2.8742e-02,  3.6501e-01, -2.5278e-01,\n",
       "                      -7.1582e-03, -2.0523e-02,  1.8663e-01, -6.4617e-02, -1.9524e-01,\n",
       "                      -3.8833e-02, -1.9442e-01, -3.3220e-01, -7.6732e-02, -1.4967e-01,\n",
       "                       2.3924e-01,  1.9432e-01, -3.0955e-01,  2.1866e-01,  6.1639e-02,\n",
       "                       7.6191e-02,  4.5339e-01,  5.4579e-01,  4.0389e-02, -3.8784e-01,\n",
       "                       1.4329e-01,  7.9312e-02, -9.7205e-02,  3.0565e-01,  3.9430e-01,\n",
       "                       1.0232e-01, -1.3421e-01,  1.2705e-01, -3.6457e-02,  4.1236e-01,\n",
       "                       3.7870e-01,  2.3225e-01, -4.6878e-01,  1.0119e-01,  3.5099e-02,\n",
       "                      -3.1020e-01, -3.0038e-01,  4.5091e-01,  1.1449e-02, -3.6010e-01,\n",
       "                       2.6860e-02, -2.1933e-01, -1.2486e-01,  2.6903e-01, -2.5644e-02,\n",
       "                      -1.1439e-01, -2.6917e-01,  2.6900e-01,  1.8133e-01,  3.5927e-01,\n",
       "                      -2.4134e-02, -3.1228e-01,  1.4701e-01,  9.8493e-02,  3.4220e-01,\n",
       "                       3.5576e-03, -4.3684e-01,  2.8155e-02, -4.8256e-01,  1.6611e-01,\n",
       "                       7.6526e-02, -2.4608e-01, -3.5448e-01,  1.4319e-01,  1.1801e-01,\n",
       "                      -3.9764e-01,  1.0978e-01,  9.9275e-02,  1.9575e-01,  6.8670e-02,\n",
       "                       4.3536e-02, -6.8916e-02, -2.3687e-01, -4.0179e-01, -6.9520e-02,\n",
       "                       2.8055e-01,  1.9178e-01,  2.2827e-01,  7.1452e-03,  1.4908e-01,\n",
       "                       2.3967e-02,  1.9457e-02,  3.1148e-01,  8.7503e-03, -3.4908e-01,\n",
       "                      -8.6206e-02,  2.9287e-01,  1.0211e-01, -3.8060e-01,  7.7693e-03,\n",
       "                       3.4794e-01,  3.2410e-01,  1.1317e-01,  1.3655e-01,  1.2104e-01,\n",
       "                      -3.2056e-02,  1.0562e-01, -1.3367e-01, -1.0236e-01,  2.2983e-01,\n",
       "                       7.8689e-02,  2.2677e-02,  2.0101e-01,  4.0412e-01, -4.9266e-02,\n",
       "                       3.5350e-01,  1.3183e-01, -1.5181e-01, -4.6391e-01, -1.9190e-01,\n",
       "                      -1.1124e-01,  2.9620e-01, -2.2563e-01, -1.5274e-01,  1.8759e-01,\n",
       "                       6.5695e-02, -1.6732e-01,  4.1691e-01,  2.9093e-01, -1.9694e-01,\n",
       "                       8.5307e-02,  1.4292e-01,  1.1915e-01, -1.6262e-01, -2.1067e-01,\n",
       "                       2.7556e-01, -4.5057e-02,  1.0585e-01, -6.5072e-02,  1.4654e-01,\n",
       "                      -2.7528e-01,  2.1685e-02, -3.2680e-01, -2.1452e-02,  2.1715e-01,\n",
       "                      -3.0908e-01,  1.0543e-01, -4.4731e-01, -1.9188e-01,  2.4431e-02,\n",
       "                      -2.8426e-02,  4.0286e-01, -2.0642e-01,  9.9660e-02, -2.3526e-01,\n",
       "                       4.5460e-02, -6.9662e-02,  3.2972e-02, -1.9804e-01, -2.2565e-01,\n",
       "                       3.3111e-01, -1.0516e-01, -2.4125e-01, -1.0311e-01, -2.8296e-01,\n",
       "                      -3.1255e-01,  2.2176e-01, -2.2037e-02,  1.3477e-02,  9.6063e-03,\n",
       "                       2.8186e-01,  1.6318e-01, -4.6143e-01,  5.0753e-02, -5.7900e-02,\n",
       "                      -9.6861e-04, -1.0669e-01,  4.0709e-01,  4.2000e-02,  1.8248e-02,\n",
       "                       3.2390e-01, -1.6889e-01, -3.8165e-01, -1.5655e-01,  6.9126e-02,\n",
       "                       8.2263e-02, -6.2033e-02, -6.9138e-02, -1.1120e-01,  1.1504e-01,\n",
       "                       4.0237e-01, -2.8517e-01, -2.6522e-01, -5.8387e-02, -2.5397e-02,\n",
       "                      -4.3828e-02,  3.3405e-02,  1.9557e-02,  6.0109e-02, -2.5014e-02,\n",
       "                      -3.3814e-01, -2.8371e-01, -3.6160e-02,  3.5369e-01, -3.2872e-02,\n",
       "                       1.4041e-01, -2.2521e-01,  1.7493e-01,  9.3309e-02,  1.7102e-01,\n",
       "                       2.5739e-03, -2.0224e-01, -8.0138e-02,  1.2354e-01, -1.7324e-02,\n",
       "                       6.8754e-02,  1.3788e-01,  1.8805e-01,  1.9993e-01, -4.0896e-01,\n",
       "                       2.1453e-01,  5.0083e-02, -9.6435e-02,  1.3969e-01,  2.8014e-02,\n",
       "                       1.1353e-01,  1.0438e-01,  3.5303e-01, -1.9488e-01,  2.1762e-01,\n",
       "                      -9.3529e-02, -5.2501e-02,  1.1169e-01,  4.7307e-02,  2.6813e-01,\n",
       "                      -1.5545e-01, -9.1192e-02,  1.1789e-01,  8.1925e-02, -4.3467e-01,\n",
       "                       2.8618e-01, -1.5985e-01,  8.2604e-02,  2.6662e-01,  1.9223e-01,\n",
       "                       3.3622e-01,  6.6544e-02, -1.2059e-01,  1.2871e-01,  8.2663e-02,\n",
       "                       2.0465e-01,  1.5216e-01,  4.7384e-02, -3.4786e-01,  2.3619e-01,\n",
       "                       4.5141e-01,  8.4367e-02, -2.9819e-01,  1.2378e-01, -1.2730e-01,\n",
       "                      -6.4680e-02, -8.7508e-02,  3.6334e-01,  3.4510e-01,  1.7252e-01,\n",
       "                       2.6669e-01, -1.3289e-01,  2.4007e-01, -3.0404e-01, -1.0779e-01,\n",
       "                       5.1013e-02, -2.1090e-01,  2.3973e-01, -7.7904e-02,  1.4247e-01,\n",
       "                       1.0473e-01,  2.6889e-01], grad_fn=<SubBackward0>)),\n",
       "             ('layer2.weight',\n",
       "              tensor([[ 0.0455, -0.0726, -0.0290,  ..., -0.0873, -0.0694, -0.0180],\n",
       "                      [ 0.0363, -0.0509,  0.0163,  ..., -0.0812,  0.0295,  0.0262],\n",
       "                      [ 0.0327, -0.0379, -0.0389,  ..., -0.0005,  0.0002,  0.0627],\n",
       "                      ...,\n",
       "                      [-0.0270, -0.0076,  0.0264,  ..., -0.0797, -0.0249, -0.0257],\n",
       "                      [ 0.0354,  0.0530, -0.0397,  ...,  0.1033,  0.0580,  0.0315],\n",
       "                      [ 0.0906,  0.0477,  0.0276,  ...,  0.0591, -0.0226,  0.1596]],\n",
       "                     grad_fn=<SubBackward0>)),\n",
       "             ('layer2.bias',\n",
       "              tensor([ 5.7085e-02,  3.6545e-02,  5.0540e-02, -3.9463e-02,  6.4612e-03,\n",
       "                      -2.8355e-02, -2.1260e-02, -2.1466e-02, -2.1996e-02, -4.5108e-03,\n",
       "                      -5.2806e-02, -4.5241e-02, -1.9098e-02,  6.1331e-03, -8.3699e-03,\n",
       "                       3.0307e-02, -1.2150e-03,  6.0530e-03, -5.5474e-02,  1.5271e-02,\n",
       "                       3.2718e-02, -7.9827e-03, -3.1075e-02,  4.5770e-02, -4.6038e-03,\n",
       "                      -8.7972e-02, -7.7515e-03, -3.0088e-03, -6.1774e-02,  7.2519e-02,\n",
       "                       8.2959e-02,  2.3718e-02,  2.2694e-02,  2.5674e-02,  1.9728e-02,\n",
       "                      -4.6010e-02,  3.2709e-02,  1.0275e-02,  8.0754e-03, -6.3316e-02,\n",
       "                      -5.3613e-02,  4.5631e-03,  3.2919e-02,  7.4033e-02, -4.3822e-02,\n",
       "                      -9.3117e-03, -1.5337e-02,  2.3294e-02,  3.7819e-02, -7.8966e-02,\n",
       "                       2.5670e-02,  3.4237e-02,  3.6486e-02,  3.0090e-02,  5.7820e-02,\n",
       "                      -5.7169e-02,  6.5565e-02, -1.8329e-02, -8.5230e-03,  2.5040e-02,\n",
       "                      -6.0057e-02, -5.4025e-02, -1.0933e-02, -1.8546e-02,  2.6877e-02,\n",
       "                       3.1852e-02,  1.1006e-03, -5.0647e-02, -4.9165e-02, -4.8181e-02,\n",
       "                      -3.7240e-02,  6.5654e-02,  3.5118e-02,  7.0554e-03, -3.4927e-02,\n",
       "                       4.1712e-02,  4.2403e-02,  9.3558e-02,  3.7414e-02, -7.5789e-02,\n",
       "                      -2.6940e-02, -6.7766e-02, -3.8108e-02,  4.7031e-03,  3.4826e-03,\n",
       "                      -1.0912e-02, -2.0044e-02, -2.7849e-02, -9.2896e-03, -1.2698e-02,\n",
       "                       4.8358e-02, -2.9116e-04, -8.8474e-03,  1.5479e-02, -5.1641e-02,\n",
       "                      -2.0225e-02,  1.7406e-02, -5.5063e-02, -2.6470e-02, -6.3804e-02,\n",
       "                       4.7441e-02, -2.0809e-02, -4.7478e-02,  1.2734e-02,  2.5001e-02,\n",
       "                      -1.3362e-02,  1.9356e-02,  2.8866e-02, -2.4095e-02, -2.5308e-02,\n",
       "                      -4.6223e-02, -3.2810e-02,  2.9938e-03, -4.4115e-02, -9.4307e-03,\n",
       "                      -4.4600e-02,  8.2016e-04, -2.4502e-02, -6.3426e-02,  4.1975e-02,\n",
       "                      -4.6404e-02, -2.9046e-02, -3.4183e-02, -7.0619e-02, -3.0225e-02,\n",
       "                      -1.9470e-02,  4.5500e-02,  4.5087e-02, -5.4761e-03,  9.2843e-02,\n",
       "                       1.0528e-02, -3.3683e-02, -5.3824e-04, -7.2277e-03, -2.7555e-03,\n",
       "                       3.1589e-02,  3.9176e-02,  1.3870e-03,  3.9075e-02, -1.8228e-02,\n",
       "                       1.6719e-02,  5.3545e-02,  2.3818e-02,  8.0862e-02,  3.1271e-02,\n",
       "                       6.6192e-03,  6.4939e-03,  1.8892e-02,  2.4225e-02,  5.0952e-02,\n",
       "                      -3.7885e-02, -3.8727e-02, -7.0335e-02,  2.2920e-02, -4.1519e-03,\n",
       "                      -8.2367e-03, -1.2136e-02, -1.8938e-02, -6.0371e-02,  9.9729e-02,\n",
       "                      -3.2573e-02, -4.1373e-03, -8.3858e-02,  3.4940e-02, -3.9345e-03,\n",
       "                       3.8065e-03, -5.7164e-02,  2.0943e-02,  1.8064e-02,  1.9173e-03,\n",
       "                       2.7830e-02, -6.4490e-02, -1.7552e-02, -6.0807e-03, -6.2814e-02,\n",
       "                      -1.4287e-02, -2.9097e-02, -9.1432e-02, -6.4630e-05,  2.4143e-02,\n",
       "                      -3.7914e-02,  3.6354e-02, -5.3579e-02, -5.2624e-03,  2.1258e-02,\n",
       "                       1.0282e-02, -5.5102e-02,  4.1680e-02,  1.7193e-02,  5.2433e-03,\n",
       "                      -2.6533e-02,  2.5504e-02,  8.6280e-03,  2.5548e-02,  2.0172e-02,\n",
       "                       3.7749e-02, -4.1667e-02, -4.3148e-02, -5.6597e-02,  8.4461e-02,\n",
       "                       9.4978e-03, -2.4676e-02,  4.6400e-02, -1.0743e-02, -4.6431e-03,\n",
       "                       1.3605e-02,  6.4094e-03, -2.0152e-02,  2.3375e-02, -1.1952e-02,\n",
       "                       3.2307e-02,  1.0335e-02, -1.9109e-02, -8.3426e-03, -2.0192e-02,\n",
       "                      -4.6664e-02,  1.7076e-02, -2.5487e-02, -1.8386e-02,  6.6697e-02,\n",
       "                      -3.9381e-02, -7.6094e-02, -5.2063e-02,  5.9609e-03, -1.0045e-02,\n",
       "                       6.3352e-03, -7.7350e-02, -2.1350e-02,  2.7861e-02, -3.3232e-02,\n",
       "                       1.0231e-02,  2.9134e-02, -8.0344e-03,  1.6700e-02, -3.3922e-02,\n",
       "                       5.3040e-03,  1.3859e-02, -1.5970e-03,  9.7595e-03,  4.7565e-02,\n",
       "                      -4.1344e-04,  1.0112e-01,  4.7927e-02, -5.1006e-02,  2.9050e-02,\n",
       "                       2.2461e-02, -2.9473e-02, -3.4910e-04,  7.5043e-02,  4.1895e-02,\n",
       "                      -7.7098e-02,  1.4229e-02,  8.4102e-03, -6.1192e-03, -1.0168e-02,\n",
       "                       3.0312e-02], grad_fn=<SubBackward0>)),\n",
       "             ('layer3.weight',\n",
       "              tensor([[-0.0289,  0.0200, -0.0909,  ...,  0.0347,  0.0118, -0.0233],\n",
       "                      [ 0.0158, -0.0591, -0.0863,  ..., -0.0271, -0.0829, -0.0393],\n",
       "                      [-0.0221, -0.0210, -0.0353,  ...,  0.0081,  0.1318,  0.0630],\n",
       "                      ...,\n",
       "                      [-0.0314, -0.0315, -0.0495,  ...,  0.0009, -0.0084,  0.1138],\n",
       "                      [ 0.0185,  0.0621, -0.0327,  ...,  0.0322, -0.0648, -0.0246],\n",
       "                      [-0.1341, -0.0230, -0.0742,  ..., -0.0826,  0.0181,  0.0224]],\n",
       "                     grad_fn=<SubBackward0>)),\n",
       "             ('layer3.bias',\n",
       "              tensor([-0.0378, -0.0440,  0.0155, -0.0043,  0.0391,  0.0271,  0.0028,  0.0318,\n",
       "                       0.0209,  0.0262,  0.0071, -0.0028, -0.0601, -0.0526, -0.0169, -0.1264,\n",
       "                      -0.0388,  0.0220,  0.0967,  0.0510,  0.0145, -0.0441, -0.0156,  0.0326,\n",
       "                       0.0169, -0.0329,  0.0657, -0.0393, -0.0338,  0.0531, -0.0581, -0.0040,\n",
       "                      -0.0167,  0.0106, -0.0034, -0.0256,  0.0338, -0.0396,  0.0765,  0.0679,\n",
       "                       0.0833, -0.0446, -0.0832,  0.0067,  0.0842, -0.0371, -0.0226,  0.0945,\n",
       "                      -0.0363, -0.0198, -0.0374, -0.0391, -0.0837,  0.0516, -0.0192, -0.0134,\n",
       "                      -0.0744, -0.0003, -0.0322,  0.0128], grad_fn=<SubBackward0>)),\n",
       "             ('output.weight',\n",
       "              tensor([[ 0.2010,  0.0127, -0.2040,  0.1528,  0.1041, -0.0461, -0.1079,  0.0621,\n",
       "                       -0.2011,  0.0298,  0.1463,  0.0431, -0.0587, -0.1831,  0.1428, -0.0779,\n",
       "                       -0.2575,  0.0968,  0.1345,  0.0569, -0.1010, -0.0675, -0.0085, -0.1803,\n",
       "                        0.0368, -0.0037, -0.1943,  0.1646,  0.2426, -0.1211, -0.0587,  0.0398,\n",
       "                        0.0357, -0.0666, -0.0923,  0.1063, -0.1573, -0.1018,  0.2649, -0.0314,\n",
       "                       -0.0741, -0.1105, -0.1439,  0.0473,  0.2455, -0.0624,  0.2627,  0.0054,\n",
       "                        0.0244,  0.1028,  0.1573, -0.1182,  0.1242, -0.1419,  0.0353, -0.1074,\n",
       "                       -0.0227, -0.0115,  0.0302,  0.1274]], grad_fn=<SubBackward0>)),\n",
       "             ('output.bias', tensor([-0.1161], grad_fn=<SubBackward0>))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425e00d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca9e5492",
   "metadata": {},
   "source": [
    "# Update HyperNetWork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9159d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "# from argparse import Namespace\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from path import Path\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "from utils.models_folktable import  DeepNet\n",
    "from utils.util import (\n",
    "    LOG_DIR,\n",
    "    TEMP_DIR,\n",
    "    clone_parameters,\n",
    "    fix_random_seed,\n",
    "    get_client_id_indices,\n",
    ")\n",
    "# from client.base import ClientBase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9252617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClientBase:\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: torch.nn.Module,\n",
    "        dataset: str,\n",
    "        batch_size: int,\n",
    "        valset_ratio: float,\n",
    "        testset_ratio: float,\n",
    "        local_epochs: int,\n",
    "        local_lr: float,\n",
    "        logger: Console,\n",
    "        gpu: int,\n",
    "    ):\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if gpu and torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.client_id: int = None\n",
    "        self.valset: DataLoader = None\n",
    "        self.trainset: DataLoader = None\n",
    "        self.testset: DataLoader = None\n",
    "            \n",
    "        name_of_model = '../../WW_WM_BW.pth'\n",
    "        init_model = DeepNet()\n",
    "        init_model.load_state_dict(torch.load(name_of_model))\n",
    "        \n",
    "     \n",
    "        # need to change\n",
    "        self.model: torch.nn.Module = init_model\n",
    "            \n",
    "        self.optimizer: torch.optim.Optimizer = torch.optim.SGD(\n",
    "            self.model.parameters(), lr=local_lr\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "        self.valset_ratio = valset_ratio\n",
    "        self.testset_ratio = testset_ratio\n",
    "        self.local_epochs = local_epochs\n",
    "        self.local_lr = local_lr\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.logger = logger\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        size = 0\n",
    "        loss = 0\n",
    "        correct = 0\n",
    "        for x, y in self.testset:\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "            logits = self.model(x)\n",
    "\n",
    "            loss += self.criterion(logits, y)\n",
    "\n",
    "            pred = torch.softmax(logits, -1).argmax(-1)\n",
    "\n",
    "            correct += (pred == y).int().sum()\n",
    "\n",
    "            size += y.size(-1)\n",
    "\n",
    "        acc = correct / size * 100.0\n",
    "        loss = loss / len(self.testset)\n",
    "        return loss, acc\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def _train(self):\n",
    "        pass\n",
    "\n",
    "    def get_client_local_dataset(self):\n",
    "        datasets = get_dataloader(\n",
    "            self.dataset,\n",
    "            self.client_id,\n",
    "            self.batch_size,\n",
    "            self.valset_ratio,\n",
    "            self.testset_ratio,\n",
    "        )\n",
    "        self.trainset = datasets[\"train\"]\n",
    "        self.valset = datasets[\"val\"]\n",
    "        self.testset = datasets[\"test\"]\n",
    "\n",
    "    def _log_while_training(self, evaluate=True, verbose=False):\n",
    "        def _log_and_train(*args, **kwargs):\n",
    "            loss_before = 0\n",
    "            loss_after = 0\n",
    "            acc_before = 0\n",
    "            acc_after = 0\n",
    "            if evaluate:\n",
    "                loss_before, acc_before = self.evaluate()\n",
    "\n",
    "            res = self._train(*args, **kwargs)\n",
    "\n",
    "            if evaluate:\n",
    "                loss_after, acc_after = self.evaluate()\n",
    "\n",
    "            if verbose:\n",
    "                self.logger.log(\n",
    "                    \"client [{}]   [bold red]loss: {:.4f} -> {:.4f}    [bold blue]accuracy: {:.2f}% -> {:.2f}%\".format(\n",
    "                        self.client_id, loss_before, loss_after, acc_before, acc_after\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            eval_stats = {\n",
    "                \"loss_before\": loss_before,\n",
    "                \"loss_after\": loss_after,\n",
    "                \"acc_before\": acc_before,\n",
    "                \"acc_after\": acc_after,\n",
    "            }\n",
    "            return res, eval_stats\n",
    "\n",
    "        return _log_and_train\n",
    "\n",
    "    def set_parameters(self, model_params: OrderedDict):\n",
    "        self.model.load_state_dict(model_params, strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81e9a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import OrderedDict\n",
    "\n",
    "import torch\n",
    "from rich.console import Console\n",
    "from utils.util import clone_parameters\n",
    "\n",
    "# from client.base import ClientBase\n",
    "\n",
    "class pFedLAClient(ClientBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: torch.nn.Module,        \n",
    "        dataset: str,\n",
    "        batch_size: int,\n",
    "        valset_ratio: float,\n",
    "        testset_ratio: float,\n",
    "        local_epochs: int,\n",
    "        local_lr: float,\n",
    "        logger: Console,\n",
    "        gpu: int,\n",
    "    ):\n",
    "        super(pFedLAClient, self).__init__(\n",
    "            backbone,\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            valset_ratio,\n",
    "            testset_ratio,\n",
    "            local_epochs,\n",
    "            logger,\n",
    "            local_lr,\n",
    "            gpu,\n",
    "        )\n",
    "        \n",
    "    def train(\n",
    "        self,\n",
    "        client_id: int,\n",
    "        model_params: OrderedDict[str, torch.Tensor],\n",
    "        verbose=True,\n",
    "    ):\n",
    "        self.client_id = client_id\n",
    "        self.set_parameters(model_params)\n",
    "        self.get_client_local_dataset()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        res, stats = self._log_while_training(evaluate=True, verbose=verbose)()\n",
    "        self.model.cpu()\n",
    "        return res, stats\n",
    "        \n",
    "    def _train(self):\n",
    "        self.model.train()\n",
    "        frz_model_params = clone_parameters(self.model)\n",
    "\n",
    "        for _ in range(self.local_epochs):\n",
    "            for x, y in self.trainset:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                logits = self.model(x)\n",
    "\n",
    "                loss = self.criterion(logits, y)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        delta = OrderedDict(\n",
    "            {\n",
    "                k: p1 - p0 \n",
    "                for (k, p1), p0 in zip(\n",
    "                    self.model.state_dict(keep_vars=True).items(),\n",
    "                    frz_model_params.values(),\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return delta\n",
    "    \n",
    "    def test(\n",
    "        self, client_id: int, model_params: OrderedDict[str, torch.Tensor],\n",
    "    ):\n",
    "        self.client_id = client_id\n",
    "        self.set_parameters(model_params)\n",
    "        self.get_client_local_dataset()\n",
    "        self.model.to(self.device)\n",
    "        loss, acc = self.evaluate()\n",
    "        dummy_diff = OrderedDict(\n",
    "            {\n",
    "                name: torch.zeros_like(param)\n",
    "                for name, param in self.model.state_dict().items()\n",
    "            }\n",
    "        )\n",
    "        self.model.cpu()\n",
    "        stats = {\"loss\": loss, \"acc\": acc}\n",
    "        return dummy_diff, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bedb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db83fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "from argparse import Namespace\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from path import Path\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.util import (\n",
    "    LOG_DIR,\n",
    "    TEMP_DIR,\n",
    "    clone_parameters,\n",
    "    fix_random_seed,\n",
    "    get_client_id_indices,\n",
    ")\n",
    "\n",
    "class ServerBase:\n",
    "    def __init__(self, args: Namespace, algo: str):\n",
    "        self.algo = \"PFedLA\"\n",
    "        self.args = args\n",
    "        \n",
    "        # default log file format\n",
    "        self.log_name = \"{}_{}_{}_{}.html\".format(\n",
    "            self.algo,\n",
    "            self.args.dataset,\n",
    "            self.args.global_epochs,\n",
    "            self.args.local_epochs,\n",
    "        )\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        fix_random_seed(5)\n",
    "        self.global_epochs=5\n",
    "        self.backbone= (DeepNet)\n",
    "        \n",
    "        \n",
    "        self.logger = Console(record=True, log_path=False, log_time=False,)\n",
    "        self.client_id_indices, self.client_num_in_total = [0,1,2,3], 2\n",
    "        \n",
    "        self.temp_dir =\"pFedLA\"\n",
    "        \n",
    "        if not os.path.isdir(self.temp_dir):\n",
    "            os.makedirs(self.temp_dir)\n",
    "\n",
    "        name_of_model = '../../WW_WM_BW.pth'\n",
    "        init_model = DeepNet()\n",
    "        init_model.load_state_dict(torch.load(name_of_model))\n",
    "        \n",
    "        _dummy_model = init_model\n",
    "\n",
    "        passed_epoch = 0\n",
    "        self.global_params_dict: OrderedDict[str : torch.Tensor] = None\n",
    "        \n",
    "        if os.listdir(self.temp_dir) != []:\n",
    "            if os.path.exists(self.temp_dir / \"global_model.pt\"):\n",
    "                self.global_params_dict = torch.load(self.temp_dir / \"global_model.pt\")\n",
    "                self.logger.log(\"Find existed global model...\")\n",
    "\n",
    "            if os.path.exists(self.temp_dir / \"epoch.pkl\"):\n",
    "                with open(self.temp_dir / \"epoch.pkl\", \"rb\") as f:\n",
    "                    passed_epoch = pickle.load(f)\n",
    "                self.logger.log(f\"Have run {passed_epoch} epochs already.\",)\n",
    "        else:\n",
    "            self.global_params_dict = OrderedDict(_dummy_model.state_dict())\n",
    "\n",
    "#         self.global_epochs = self.args.global_epochs - passed_epoch\n",
    "        self.global_epochs = self.global_epochs - passed_epoch\n",
    "    \n",
    "        self.logger.log(\"Backbone:\", _dummy_model)\n",
    "\n",
    "        self.trainer: ClientBase = None\n",
    "        self.all_clients_stats = {i: {} for i in self.client_id_indices}\n",
    "           \n",
    "            \n",
    "            \n",
    "    def train(self):\n",
    "\n",
    "        print(\"In server class \\n\")\n",
    "        self.logger.log(\"=\" * 30, \"TRAINING\", \"=\" * 30, style=\"bold green\")\n",
    "        progress_bar = (\n",
    "            track(\n",
    "                range(self.global_epochs),\n",
    "                \"[bold green]Training...\",\n",
    "                console=self.logger,\n",
    "            )\n",
    "            if not self.args.log\n",
    "            else tqdm(range(self.global_epochs), \"Training...\")\n",
    "        )\n",
    "        for E in progress_bar:\n",
    "\n",
    "#             if E % self.args.verbose_gap == 0:\n",
    "#                 self.logger.log(\"=\" * 30, f\"ROUND: {E}\", \"=\" * 30)\n",
    "\n",
    "            selected_clients = random.sample(\n",
    "                self.client_id_indices, self.args.client_num_per_round\n",
    "            )\n",
    "            \n",
    "            updated_params_cache = []\n",
    "            weights_cache = []\n",
    "\n",
    "            for client_id in selected_clients:\n",
    "                client_local_params = clone_parameters(self.global_params_dict)\n",
    "                (updated_params, weight), stats = self.trainer.train(\n",
    "                    client_id=client_id,\n",
    "                    model_params=client_local_params,\n",
    "                    verbose=(E % self.args.verbose_gap) == 0,\n",
    "                )\n",
    "\n",
    "                updated_params_cache.append(updated_params)\n",
    "                weights_cache.append(weight)\n",
    "                self.all_clients_stats[client_id][f\"ROUND: {E}\"] = (\n",
    "                    f\"{stats['loss_before']:.4f} -> {stats['loss_after']:.4f}\",\n",
    "                )\n",
    "\n",
    "            self.aggregate_parameters(updated_params_cache, weights_cache)\n",
    "\n",
    "            if E % self.args.save_period == 0:\n",
    "                torch.save(\n",
    "                    self.global_params_dict, self.temp_dir / \"global_model.pt\",\n",
    "                )\n",
    "                with open(self.temp_dir / \"epoch.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(E, f)\n",
    "        self.logger.log(self.all_clients_stats)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def aggregate_parameters(self, updated_params_cache, weights_cache):\n",
    "        weight_sum = sum(weights_cache)\n",
    "        weights = torch.tensor(weights_cache, device=self.device) / weight_sum\n",
    "\n",
    "        aggregated_params = []\n",
    "\n",
    "        for params in zip(*updated_params_cache):\n",
    "            aggregated_params.append(\n",
    "                torch.sum(weights * torch.stack(params, dim=-1), dim=-1)\n",
    "            )\n",
    "\n",
    "        self.global_params_dict = OrderedDict(\n",
    "            zip(self.global_params_dict.keys(), aggregated_params)\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "    def test(self) -> None:\n",
    "        self.logger.log(\"=\" * 30, \"TESTING\", \"=\" * 30, style=\"bold blue\")\n",
    "        all_loss = []\n",
    "        all_acc = []\n",
    "        for client_id in track(\n",
    "            self.client_id_indices,\n",
    "            \"[bold blue]Testing...\",\n",
    "            console=self.logger,\n",
    "            disable=self.args.log,\n",
    "        ):\n",
    "            client_local_params = clone_parameters(self.global_params_dict)\n",
    "            stats = self.trainer.test(\n",
    "                client_id=client_id, model_params=client_local_params,\n",
    "            )\n",
    "\n",
    "            self.logger.log(\n",
    "                f\"client [{client_id}]  [red]loss: {stats['loss']:.4f}    [magenta]accuracy: {stats['acc']:.2f}%\"\n",
    "            )\n",
    "            all_loss.append(stats[\"loss\"])\n",
    "            all_acc.append(stats[\"acc\"])\n",
    "\n",
    "        self.logger.log(\"=\" * 20, \"RESULTS\", \"=\" * 20, style=\"bold green\")\n",
    "        self.logger.log(\n",
    "            \"loss: {:.4f}    accuracy: {:.2f}%\".format(\n",
    "                sum(all_loss) / len(all_loss), sum(all_acc) / len(all_acc),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "#         self.logger.log(\"Arguments:\", dict(self.args._get_kwargs()))\n",
    "        self.train()\n",
    "        self.test()\n",
    "        if self.args.log:\n",
    "            if not os.path.isdir(LOG_DIR):\n",
    "                os.mkdir(LOG_DIR)\n",
    "            self.logger.save_html(LOG_DIR / self.log_name)\n",
    "\n",
    "        # delete all temporary files\n",
    "        if os.listdir(self.temp_dir) != []:\n",
    "            os.system(f\"rm -rf {self.temp_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2144341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9329f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.empty(out_features))\n",
    "\n",
    "        nn.init.uniform_(self.weight)\n",
    "        nn.init.constant_(self.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight, self.bias)\n",
    "\n",
    "    \n",
    "\n",
    "class HyperNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        backbone: nn.Module,\n",
    "        client_num: int=4,\n",
    "        K: int=2,\n",
    "        gpu=True,\n",
    "    ):\n",
    "    \n",
    "        super(HyperNetwork, self).__init__()\n",
    "        self.device = torch.device(\n",
    "                \"cuda\" if gpu and torch.cuda.is_available() else \"cpu\"\n",
    "            )\n",
    "        \n",
    "        self.K = K\n",
    "        self.client_num = client_num\n",
    "        self.embedding = nn.Embedding(client_num, embedding_dim, device=self.device)\n",
    "        self.blocks_name = set(n.split(\".\")[0] for n, _ in backbone.named_parameters())\n",
    "        self.cache_dir =  \"pkl_files/hn\"  # put dir here\n",
    "        \n",
    "        if os.listdir(self.cache_dir) != client_num:\n",
    "            \n",
    "            for client_id in range(client_num):\n",
    "#                 with open(self.cache_dir / f\"{client_id}.pkl\", \"wb\") as f:\n",
    "                with open(os.path.join(self.cache_dir, f\"{client_id}.pkl\"), \"wb\") as f:\n",
    "\n",
    "                    pickle.dump(\n",
    "                        {\n",
    "                            \"mlp\": nn.Sequential(\n",
    "                                nn.Linear(embedding_dim, hidden_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(hidden_dim, hidden_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(hidden_dim, hidden_dim),\n",
    "                                nn.ReLU(),\n",
    "                            ),\n",
    "                            \n",
    "                            \"fc\": {\n",
    "                                name: Linear(hidden_dim, client_num)\n",
    "                                for name in self.blocks_name\n",
    "                            },\n",
    "                        },\n",
    "                        f,\n",
    "                    )\n",
    "\n",
    "        # for tracking the current client's hn parameters\n",
    "        self.current_client_id: int = None\n",
    "        self.mlp: nn.Sequential = None\n",
    "        self.fc_layers: Dict[str, Linear] = {}\n",
    "        self.retain_blocks: List[str] = []\n",
    "            \n",
    "        print(\"HypterNetwork\")\n",
    "        \n",
    "    def mlp_parameters(self) -> List[nn.Parameter]:\n",
    "            print(\"self.mlp.parameters():: \", self.mlp)            \n",
    "            return list(filter(lambda p: p.requires_grad, self.mlp.parameters()))        \n",
    "        \n",
    "    def save_hn(self):\n",
    "            for block, param in self.fc_layers.items():\n",
    "                self.fc_layers[block] = param.cpu()\n",
    "            with open(self.cache_dir / f\"{self.current_client_id}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(\n",
    "                    {\"mlp\": self.mlp.cpu(), \"fc\": self.fc_layers}, f,\n",
    "                )\n",
    "            self.mlp = None\n",
    "            self.fc_layers = {}\n",
    "            self.current_client_id = None\n",
    "\n",
    "    def load_hn(self) -> Tuple[nn.Sequential, OrderedDict[str, Linear]]:\n",
    "        with open(self.cache_dir / f\"{self.current_client_id}.pkl\", \"rb\") as f:\n",
    "            parameters = pickle.load(f)\n",
    "        self.mlp = parameters[\"mlp\"].to(self.device)\n",
    "        for block, param in parameters[\"fc\"].items():\n",
    "            self.fc_layers[block] = param.to(self.device)\n",
    "\n",
    "    def clean_models(self):\n",
    "        if os.path.isdir(self.cache_dir):\n",
    "            os.system(f\"rm -rf {self.cache_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d50d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Backbone: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DeepNet</span><span style=\"font-weight: bold\">(</span>                                                                                                 \n",
       "  <span style=\"font-weight: bold\">(</span>layer1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>                                                    \n",
       "  <span style=\"font-weight: bold\">(</span>act1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>                                                                                                   \n",
       "  <span style=\"font-weight: bold\">(</span>dropout1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                                                        \n",
       "  <span style=\"font-weight: bold\">(</span>layer2<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>                                                   \n",
       "  <span style=\"font-weight: bold\">(</span>act2<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>                                                                                                   \n",
       "  <span style=\"font-weight: bold\">(</span>layer3<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>                                                    \n",
       "  <span style=\"font-weight: bold\">(</span>act3<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>                                                                                                   \n",
       "  <span style=\"font-weight: bold\">(</span>output<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>                                                      \n",
       "  <span style=\"font-weight: bold\">(</span>sigmoid<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Sigmoid</span><span style=\"font-weight: bold\">()</span>                                                                                             \n",
       "<span style=\"font-weight: bold\">)</span>                                                                                                                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Backbone: \u001b[1;35mDeepNet\u001b[0m\u001b[1m(\u001b[0m                                                                                                 \n",
       "  \u001b[1m(\u001b[0mlayer1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m14\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                                    \n",
       "  \u001b[1m(\u001b[0mact1\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                                                                   \n",
       "  \u001b[1m(\u001b[0mdropout1\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                                                        \n",
       "  \u001b[1m(\u001b[0mlayer2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                                   \n",
       "  \u001b[1m(\u001b[0mact2\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                                                                   \n",
       "  \u001b[1m(\u001b[0mlayer3\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m60\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                                    \n",
       "  \u001b[1m(\u001b[0mact3\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                                                                   \n",
       "  \u001b[1m(\u001b[0moutput\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m60\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                                      \n",
       "  \u001b[1m(\u001b[0msigmoid\u001b[1m)\u001b[0m: \u001b[1;35mSigmoid\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                                                             \n",
       "\u001b[1m)\u001b[0m                                                                                                                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from typing import List, OrderedDict, Tuple\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "\n",
    "# from server.base import ServerBase\n",
    "# from client.pFedLA import pFedLAClient\n",
    "from tqdm import tqdm\n",
    "# from utils.args import get_pFedLA_args\n",
    "\n",
    "args = Namespace(\n",
    "    k=2,\n",
    "    global_epochs=5,\n",
    "    local_epochs=5,\n",
    "    local_lr=1e-2,\n",
    "    hn_lr=5e-3,\n",
    "    verbose_gap=20,\n",
    "    embedding_dim=10,\n",
    "    hidden_dim=10,\n",
    "    dataset=\"cifar10\",\n",
    "    batch_size=32,\n",
    "    valset_ratio=0.0,\n",
    "    testset_ratio=0.3,\n",
    "    gpu=1,\n",
    "    log=0,\n",
    "    seed=5,\n",
    "    client_num_per_round=4,\n",
    "    save_period=5,\n",
    ")\n",
    "\n",
    "\n",
    "class pFedLAServer(ServerBase):\n",
    "    def __init__(self):\n",
    "        super(pFedLAServer, self).__init__(args, \"pFedLA\")\n",
    "        \n",
    "        self.log_name = \"{}_{}_{}_{}_{}.html\".format(\n",
    "            self.algo,\n",
    "            self.args.dataset,\n",
    "            self.args.global_epochs,\n",
    "            self.args.local_epochs,\n",
    "            self.args.k,\n",
    "        )\n",
    "        \n",
    "        name_of_model = '../../WW_WM_BW.pth'\n",
    "        init_model = DeepNet()\n",
    "        init_model.load_state_dict(torch.load(name_of_model))\n",
    "        \n",
    "        #need to change\n",
    "        _dummy_model = init_model\n",
    "        self.client_model_params_list = init_model.load_state_dict(torch.load(name_of_model))\n",
    "        \n",
    "        self.hypernet = HyperNetwork(\n",
    "            client_num=3,\n",
    "            backbone=_dummy_model,\n",
    "            embedding_dim=10,\n",
    "            hidden_dim=10,\n",
    "            K=2,\n",
    "            gpu=True,\n",
    "        )\n",
    "        \n",
    "        self.trainer = pFedLAClient(\n",
    "            backbone=_dummy_model,\n",
    "            dataset=[\"cifar10\"],\n",
    "            batch_size=32,\n",
    "            valset_ratio=0.0,\n",
    "            testset_ratio=0.3,\n",
    "            local_epochs=5,\n",
    "            local_lr=1e-2,\n",
    "            logger=0,\n",
    "            gpu=True,\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.all_params_name = [name for name in _dummy_model.state_dict().keys()]\n",
    "        self.trainable_params_name = [\n",
    "                name\n",
    "                for name, param in _dummy_model.state_dict(keep_vars=True).items()\n",
    "                if param.requires_grad\n",
    "            ]\n",
    "\n",
    "    def train(self) -> None:\n",
    "        self.logger.log(\"=\" * 30, \"TRAINING\", \"=\" * 30, style=\"bold green\")\n",
    "        progress_bar = (\n",
    "            track(\n",
    "                range(self.global_epochs),\n",
    "                \"[bold green]Training...\",\n",
    "                console=self.logger,\n",
    "            )\n",
    "            if not self.args.log\n",
    "            else tqdm(range(self.global_epochs), \"Training...\")\n",
    "        )\n",
    "        print(\"hello\")\n",
    "        for E in progress_bar:\n",
    "\n",
    "            if E % self.args.verbose_gap == 0:\n",
    "                self.logger.log(\"=\" * 30, f\"ROUND: {E}\", \"=\" * 30)\n",
    "\n",
    "            selected_clients = random.sample(\n",
    "                self.client_id_indices, self.args.client_num_per_round\n",
    "            )\n",
    "            for client_id in selected_clients:\n",
    "                (\n",
    "                    client_local_params,\n",
    "                    retain_blocks,\n",
    "                ) = self.generate_client_model_parameters(client_id)\n",
    "\n",
    "                diff, stats = self.trainer.train(\n",
    "                    client_id=client_id,\n",
    "                    model_params=client_local_params,\n",
    "                    verbose=(E % self.args.verbose_gap) == 1,\n",
    "                )\n",
    "                \n",
    "                self.all_clients_stats[client_id][f\"ROUND: {E}\"] = (\n",
    "                    f\"retain {retain_blocks}, {stats['loss_before']:.4f} -> {stats['loss_after']:.4f}\",\n",
    "                )\n",
    "\n",
    "                self.update_hypernetwork(client_id, diff, retain_blocks)\n",
    "\n",
    "                self.update_client_model_parameters(client_id, diff)\n",
    "\n",
    "            if E % self.args.save_period == 0:\n",
    "                torch.save(\n",
    "                    self.client_model_params_list, self.temp_dir / \"clients_model.pt\",\n",
    "                )\n",
    "                with open(self.temp_dir / \"epoch.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(E, f)\n",
    "        self.logger.log(self.all_clients_stats)\n",
    "    \n",
    "    def generate_client_model_parameters(\n",
    "            self, client_id: int\n",
    "        ) -> Tuple[OrderedDict[str, torch.Tensor], List[str]]:\n",
    "        \n",
    "        \n",
    "            layer_params_dict = dict(\n",
    "                zip(self.all_params_name, list(zip(*self.client_model_params_list)))\n",
    "            )\n",
    "\n",
    "            alpha, retain_blocks = self.hypernet(client_id)\n",
    "\n",
    "\n",
    "            aggregated_parameters = {}\n",
    "            default_weight = torch.tensor(\n",
    "                [i == client_id for i in range(self.client_num_in_total)],\n",
    "                dtype=torch.float,\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "\n",
    "            for name in self.all_params_name:\n",
    "\n",
    "                if name in self.trainable_params_name:\n",
    "\n",
    "\n",
    "                    a = alpha[name.split(\".\")[0]]\n",
    "\n",
    "                else:\n",
    "                    a = default_weight\n",
    "\n",
    "                print(\"From hypernet:  alpha \", alpha,\"\\n\")\n",
    "\n",
    "                # print(\" NAME: all_params_name AND trainable_params_name\",self.all_params_name,\" \\n \\n\",self.trainable_params_name,\"\\n\")\n",
    "\n",
    "                # print(\"Default weight A and name\", name,\"  ::  \" ,a,\"\\n\")\n",
    "\n",
    "                if a.sum() == 0:\n",
    "                    self.logger.log(self.all_clients_stats)\n",
    "                    raise RuntimeError(\n",
    "                        f\"client [{client_id}]'s {name.split('.')[0]} alpha is a all 0 vector\"\n",
    "                    )\n",
    "\n",
    "\n",
    "                # print(\"layer_params_dict[name]\", name,\" :: \",layer_params_dict[name][0],\"\\n\")\n",
    "\n",
    "                aggregated_parameters[name] = torch.sum(\n",
    "                    a\n",
    "                    / a.sum()\n",
    "                    * torch.stack(layer_params_dict[name], dim=-1).to(self.device),\n",
    "                    dim=-1,\n",
    "                )\n",
    "\n",
    "            # in the begining, its  client_model_params_list.value \n",
    "            ##putting values to  particular client's weights\n",
    "            self.client_model_params_list[client_id] = list(aggregated_parameters.values())\n",
    "\n",
    "            print(\"retain_blocks : \" , retain_blocks,\"\\n\")\n",
    "            \n",
    "            return aggregated_parameters, retain_blocks\n",
    "        \n",
    "    def update_hypernetwork(\n",
    "        self,\n",
    "        client_id: int,\n",
    "        diff: OrderedDict[str, torch.Tensor],\n",
    "        retain_blocks: List[str] = [],\n",
    "    ) -> None:\n",
    "        # calculate gradients\n",
    "        print(\"hello\")\n",
    "        inputs= self.hypernet.mlp_parameters()\n",
    "        print(inputs)\n",
    "        \n",
    "    def run(self):\n",
    "        super().run()\n",
    "        # clean out all HNs\n",
    "        self.hypernet.clean_models()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    server = pFedLAServer()\n",
    "    server.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f7388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c234e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df3b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pFedLAServer()\n",
    "\n",
    "s1.update_hypernetwork(2, delta, topk_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6800e76f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5659372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mlp': Sequential(\n",
       "   (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "   (3): ReLU()\n",
       "   (4): Linear(in_features=10, out_features=10, bias=True)\n",
       "   (5): ReLU()\n",
       " ),\n",
       " 'fc': {'layer3': Linear(),\n",
       "  'output': Linear(),\n",
       "  'layer2': Linear(),\n",
       "  'layer1': Linear()}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "with open('D:/Download/pythonProject/HiWi/pFedLA_Folktable/src/pkl_files/hn/1.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "    \n",
    "loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5086c528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b8bc52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c22274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f3064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94f483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d40434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94674a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
